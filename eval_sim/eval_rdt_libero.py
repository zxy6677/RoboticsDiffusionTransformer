#!/usr/bin/env python3
"""
RDTåœ¨LIBEROä»»åŠ¡ä¸Šçš„æ¨ç†è¯„ä¼°è„šæœ¬
åŸºäºRDTçš„æ¨ç†æµç¨‹ï¼Œé€‚é…LIBEROç¯å¢ƒ
"""

import sys
import os
import numpy as np
import torch
from PIL import Image
import yaml
import argparse
from typing import Dict, List, Tuple, Optional
import cv2
from datetime import datetime

# æ·»åŠ è·¯å¾„
script_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.abspath(os.path.dirname(script_dir))
libero_path1 = os.path.abspath(os.path.join(project_root, '..', 'LIBERO', 'libero'))
libero_path2 = os.path.abspath(os.path.join(project_root, '..', 'LIBERO', 'libero', 'libero'))

# å°†æ‰€æœ‰è·¯å¾„æ·»åŠ åˆ°sys.pathï¼ˆç¡®ä¿ä½¿ç”¨ç»å¯¹è·¯å¾„ï¼‰
for path in [libero_path1, libero_path2, project_root]:
    if path not in sys.path:
        sys.path.insert(0, path)

# å¯¼å…¥LIBEROæ¨¡å—
import libero
from libero import benchmark
from libero.envs import OffScreenRenderEnv

# å¯¼å…¥RDTæ¨¡å—
import importlib.util

# åŠ¨æ€å¯¼å…¥configs.state_vec
state_vec_path = os.path.join(project_root, "configs", "state_vec.py")
spec = importlib.util.spec_from_file_location("state_vec", state_vec_path)
state_vec_module = importlib.util.module_from_spec(spec)
sys.modules["state_vec"] = state_vec_module
spec.loader.exec_module(state_vec_module)
STATE_VEC_IDX_MAPPING = state_vec_module.STATE_VEC_IDX_MAPPING

from models.multimodal_encoder.siglip_encoder import SiglipVisionTower
from models.multimodal_encoder.t5_encoder import T5Embedder
from models.rdt_runner import RDTRunner
from utils.rotation_utils import convert_quaternion_to_6d_rotation, convert_6d_rotation_to_euler

class VideoRecorder:
    """è§†é¢‘å½•åˆ¶å™¨"""
    
    def __init__(self, output_path: str, fps: int = 30, width: int = 128, height: int = 128):
        """åˆå§‹åŒ–è§†é¢‘å½•åˆ¶å™¨"""
        self.output_path = output_path
        self.fps = fps
        self.width = width
        self.height = height
        self.frames = []
        
    def add_frame(self, image: np.ndarray):
        """æ·»åŠ å¸§"""
        if image is not None:
            # ç¡®ä¿å›¾åƒå°ºå¯¸æ­£ç¡®
            if image.shape[:2] != (self.height, self.width):
                image = cv2.resize(image, (self.width, self.height))
            self.frames.append(image)
    
    def save_video(self):
        """ä¿å­˜è§†é¢‘"""
        if not self.frames:
            print("âš ï¸ æ²¡æœ‰å¸§å¯ä»¥ä¿å­˜")
            return
            
        # åˆ›å»ºè§†é¢‘å†™å…¥å™¨
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(self.output_path, fourcc, self.fps, (self.width, self.height))
        
        for frame in self.frames:
            # ç¡®ä¿å¸§æ˜¯BGRæ ¼å¼
            if len(frame.shape) == 3 and frame.shape[2] == 3:
                # å‡è®¾æ˜¯RGBï¼Œè½¬æ¢ä¸ºBGR
                frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
            else:
                frame_bgr = frame
            out.write(frame_bgr)
        
        out.release()
        print(f"ğŸ¥ è§†é¢‘å·²ä¿å­˜åˆ°: {self.output_path}")
        
    def clear(self):
        """æ¸…ç©ºå¸§"""
        self.frames = []

class RDTLIBEROModel:
    """RDTæ¨¡å‹åœ¨LIBEROä¸Šçš„æ¨ç†åŒ…è£…å™¨"""
    
    def __init__(self, config_path: str, pretrained_path: str, 
                 text_encoder_path: str, vision_encoder_path: str):
        """åˆå§‹åŒ–RDTæ¨¡å‹"""
        self.config_path = config_path
        self.pretrained_path = pretrained_path
        self.text_encoder_path = text_encoder_path
        self.vision_encoder_path = vision_encoder_path
        
        # åŠ è½½é…ç½®
        with open(config_path, "r") as fp:
            self.config = yaml.safe_load(fp)
        
        # åˆå§‹åŒ–ç¼–ç å™¨
        self._init_encoders()
        
        # åˆå§‹åŒ–RDTæ¨¡å‹
        self._init_rdt_model()
        
    def _init_encoders(self):
        """åˆå§‹åŒ–æ–‡æœ¬å’Œè§†è§‰ç¼–ç å™¨"""
        # æ–‡æœ¬ç¼–ç å™¨
        self.text_encoder = T5Embedder(
            device="cuda",
            from_pretrained=self.text_encoder_path,
            cache_dir=None,
            model_max_length=77,
        )
        
        # è§†è§‰ç¼–ç å™¨
        self.vision_encoder = SiglipVisionTower(
            vision_tower=self.vision_encoder_path,
            args=self.config,
            delay_load=False,
        )
        
    def _init_rdt_model(self):
        """åˆå§‹åŒ–RDTæ¨¡å‹"""
        # è®¡ç®—å›¾åƒæ¡ä»¶é•¿åº¦
        # SigLIPçš„patchæ•°é‡è®¡ç®—: (image_size / patch_size)^2
        patch_size = self.vision_encoder.vision_tower.config.patch_size
        image_size = self.vision_encoder.vision_tower.config.image_size
        num_patches = (image_size // patch_size) ** 2
        
        img_cond_len = (self.config["common"]["img_history_size"] 
                       * self.config["common"]["num_cameras"] 
                       * num_patches)
        
        self.rdt_model = RDTRunner(
            action_dim=self.config["common"]["state_dim"],
            pred_horizon=self.config["common"]["action_chunk_size"],
            config=self.config["model"],
            lang_token_dim=self.config["model"]["lang_token_dim"],
            img_token_dim=self.config["model"]["img_token_dim"],
            state_token_dim=self.config["model"]["state_token_dim"],
            max_lang_cond_len=self.config["dataset"]["tokenizer_max_length"],
            img_cond_len=img_cond_len,
            img_pos_embed_config=[
                ("image", (self.config["common"]["img_history_size"], 
                    self.config["common"]["num_cameras"], 
                    -num_patches)),  
            ],
            lang_pos_embed_config=[
                ("lang", -self.config["dataset"]["tokenizer_max_length"]),
            ],
            dtype=torch.bfloat16,
        )
        
        # åŠ è½½é¢„è®­ç»ƒæƒé‡
        model_file = os.path.join(self.pretrained_path, "pytorch_model.bin")
        if os.path.exists(model_file):
            checkpoint = torch.load(model_file, map_location="cpu")
            self.rdt_model.load_state_dict(checkpoint, strict=False)
            print(f"âœ… åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {model_file}")
        else:
            print(f"âš ï¸ é¢„è®­ç»ƒæ¨¡å‹ä¸å­˜åœ¨: {model_file}")
            
    def encode_instruction(self, instruction: str) -> torch.Tensor:
        """ç¼–ç ä»»åŠ¡æŒ‡ä»¤"""
        text_embeds, attention_mask = self.text_encoder.get_text_embeddings([instruction])
        return text_embeds
    
    def reset(self):
        """é‡ç½®æ¨¡å‹çŠ¶æ€"""
        device = "cuda"
        weight_dtype = torch.bfloat16
        self.rdt_model.eval()
        self.text_encoder.model.eval()
        self.vision_encoder.vision_tower.eval()
        
        self.rdt_model = self.rdt_model.to(device, dtype=weight_dtype)
        self.text_encoder.model = self.text_encoder.model.to(device, dtype=weight_dtype)
        self.vision_encoder.vision_tower = self.vision_encoder.vision_tower.to(device, dtype=weight_dtype)
        
    def step(self, state: torch.Tensor, images: List[Image.Image], 
             text_embed: torch.Tensor) -> torch.Tensor:
        """æ‰§è¡Œä¸€æ­¥æ¨ç†"""
        device = "cuda"
        dtype = torch.bfloat16
        
        # å¤„ç†å›¾åƒ - å‚è€ƒmaniskill_model.pyçš„å®ç°
        background_color = np.array([
            int(x*255) for x in self.vision_encoder.image_processor.image_mean
        ], dtype=np.uint8).reshape(1, 1, 3)
        background_image = np.ones((
            self.vision_encoder.image_processor.size["height"], 
            self.vision_encoder.image_processor.size["width"], 3), dtype=np.uint8
        ) * background_color
        
        image_tensor_list = []
        for image in images:
            if image is None:
                # ç”¨èƒŒæ™¯å›¾åƒæ›¿æ¢
                image = Image.fromarray(background_image)
            
            # é¢„å¤„ç†å›¾åƒ
            image = self.vision_encoder.image_processor.preprocess(image, return_tensors='pt')['pixel_values'][0]
            image_tensor_list.append(image)

        image_tensor = torch.stack(image_tensor_list, dim=0).to(device, dtype=dtype)
        
        # è·å–å›¾åƒç‰¹å¾ - å¤„ç†æ¯ä¸ªå›¾åƒ
        image_embeds_list = []
        for i in range(image_tensor.shape[0]):
            single_image = image_tensor[i:i+1]  # (1, 3, 384, 384)
            vision_output = self.vision_encoder.vision_tower(single_image)
            single_embeds = vision_output.last_hidden_state.detach()  # (1, 729, 1152)
            image_embeds_list.append(single_embeds)
        
        # æ‹¼æ¥æ‰€æœ‰å›¾åƒç‰¹å¾
        image_embeds = torch.cat(image_embeds_list, dim=1)  # (1, 729*3, 1152)
        
        # ä¸ºäº†åŒ¹é…è®­ç»ƒæ—¶çš„æ ¼å¼ï¼Œæˆ‘ä»¬éœ€è¦é‡å¤å›¾åƒç‰¹å¾ä»¥æ¨¡æ‹Ÿå†å²
        # è®­ç»ƒæ—¶ä½¿ç”¨ img_history_size=2ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦é‡å¤ä¸€æ¬¡
        img_history_size = self.config["common"]["img_history_size"]
        if img_history_size > 1:
            # é‡å¤å›¾åƒç‰¹å¾ä»¥æ¨¡æ‹Ÿå†å²
            image_embeds = image_embeds.repeat(1, img_history_size, 1)  # (1, 729*3*2, 1152)
        
        # é‡å¡‘ä¸ºæ­£ç¡®çš„å½¢çŠ¶: (batch_size, num_patches, hidden_size)
        image_embeds = image_embeds.reshape(1, -1, self.vision_encoder.vision_tower.config.hidden_size)
        
        # å¤„ç†çŠ¶æ€
        joints = state.to(device).unsqueeze(0)   # (1, 128)
        states = joints.to(device, dtype=dtype).unsqueeze(1)  # (1, 1, 128)
        
        # åˆ›å»ºçŠ¶æ€æ©ç 
        state_elem_mask = torch.ones(
            (1, 1, self.config["model"]["state_token_dim"]),
            device=device, dtype=dtype
        )
        
        
        ctrl_freqs = torch.tensor([20]).to(device)  # LIBEROæ§åˆ¶é¢‘ç‡
        
        text_embeds = text_embed.to(device, dtype=dtype)
        
        with torch.no_grad():
            trajectory = self.rdt_model.predict_action(
                lang_tokens=text_embeds,
                lang_attn_mask=torch.ones(
                    text_embeds.shape[:2], dtype=torch.bool,
                    device=text_embeds.device),
                img_tokens=image_embeds,
                state_tokens=states,
                action_mask=state_elem_mask,  
                ctrl_freqs=ctrl_freqs
            )
        
        return trajectory.to(torch.float32)

def convert_libero_state_to_rdt(obs: Dict, state_dim: int = 128) -> torch.Tensor:
    """å°†LIBEROè§‚å¯Ÿè½¬æ¢ä¸ºRDTçŠ¶æ€æ ¼å¼"""
    # æå–LIBEROçŠ¶æ€
    joint_pos = obs["robot0_joint_pos"]  # (7,)
    gripper_pos = obs["robot0_gripper_qpos"]  # (2,)
    eef_pos = obs["robot0_eef_pos"]  # (3,)
    eef_quat = obs["robot0_eef_quat"]  # (4,)
    
    # è®¡ç®—gripperçŠ¶æ€
    gripper_state = np.mean(gripper_pos)
    
    # ä½¿ç”¨ä¿®å¤åçš„å››å…ƒæ•°åˆ°6Dæ—‹è½¬è½¬æ¢å‡½æ•°
    eef_ori_6d = convert_quaternion_to_6d_rotation(eef_quat)
    
    # æ„å»º17ç»´LIBEROçŠ¶æ€å‘é‡
    libero_state = np.concatenate([
        joint_pos,           # 7ç»´
        [gripper_state],     # 1ç»´
        eef_pos,            # 3ç»´
        eef_ori_6d          # 6ç»´ (æ­£ç¡®çš„6Dæ—‹è½¬)
    ])  # æ€»å…±17ç»´
    
    # æ˜ å°„åˆ°RDTçš„128ç»´çŠ¶æ€ç©ºé—´
    rdt_state = np.zeros(state_dim)
    
    # ä½¿ç”¨å³è‡‚çš„ç´¢å¼•æ˜ å°„
    right_arm_indices = [
        STATE_VEC_IDX_MAPPING[f"right_arm_joint_{i}_pos"] for i in range(7)
    ] + [
        STATE_VEC_IDX_MAPPING["right_gripper_open"]
    ] + [
        STATE_VEC_IDX_MAPPING["right_eef_pos_x"],
        STATE_VEC_IDX_MAPPING["right_eef_pos_y"], 
        STATE_VEC_IDX_MAPPING["right_eef_pos_z"]
    ] + [
        STATE_VEC_IDX_MAPPING["right_eef_angle_0"],
        STATE_VEC_IDX_MAPPING["right_eef_angle_1"],
        STATE_VEC_IDX_MAPPING["right_eef_angle_2"],
        STATE_VEC_IDX_MAPPING["right_eef_angle_3"],
        STATE_VEC_IDX_MAPPING["right_eef_angle_4"],
        STATE_VEC_IDX_MAPPING["right_eef_angle_5"]
    ]
    
    # ç¡®ä¿ç´¢å¼•æ•°é‡åŒ¹é…
    min_len = min(len(libero_state), len(right_arm_indices))
    rdt_state[right_arm_indices[:min_len]] = libero_state[:min_len]
    
    # âš ï¸ é‡è¦ï¼šè®­ç»ƒæ—¶Stateæ²¡æœ‰å½’ä¸€åŒ–ï¼Œæ‰€ä»¥è¯„ä¼°æ—¶ä¹Ÿä¸èƒ½å½’ä¸€åŒ–ï¼
    # å¦‚æœå½’ä¸€åŒ–ï¼Œæ¨¡å‹ä¼šæ¥æ”¶åˆ°å®Œå…¨ä¸åŒçš„è¾“å…¥ï¼Œå¯¼è‡´è¾“å‡ºé”™è¯¯
    # 
    # é”™è¯¯çš„åšæ³•ï¼ˆä¼šå¯¼è‡´actionå…¨æ˜¯è´Ÿå€¼ï¼‰ï¼š
    # rdt_state = (rdt_state - state_mean) / state_std
    #
    # æ­£ç¡®çš„åšæ³•ï¼šç›´æ¥ä½¿ç”¨åŸå§‹Stateå€¼
    return torch.from_numpy(rdt_state).float()

# åœ¨æ¨¡å—çº§åˆ«åŠ è½½ç»Ÿè®¡ä¿¡æ¯å’Œå¯¼å…¥utilsï¼ˆé¿å…é‡å¤ï¼‰
import json
_LIBERO_STATS = None

def _get_libero_stats():
    """è·å–LIBEROæ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯ï¼ˆç¼“å­˜ï¼‰"""
    global _LIBERO_STATS
    if _LIBERO_STATS is None:
        dataset_stat_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'configs', 'dataset_stat.json')
        with open(dataset_stat_path, 'r') as f:
            stats = json.load(f)
        _LIBERO_STATS = stats['libero_90']
    return _LIBERO_STATS

def convert_rdt_action_to_libero(rdt_action: torch.Tensor) -> np.ndarray:
    """
    å°†RDTåŠ¨ä½œï¼ˆç‰©ç†å•ä½ï¼‰è½¬æ¢ä¸ºLIBEROåŠ¨ä½œæ ¼å¼ï¼ˆå½’ä¸€åŒ–ï¼‰
    
    é‡è¦ï¼šä¿®å¤åçš„RDTè®­ç»ƒä½¿ç”¨ç‰©ç†å•ä½ï¼ˆç¬¦åˆREADME IMPORTANT 3ï¼‰
    - ä½ç½®ï¼šç±³ï¼ˆç‰©ç†å•ä½ï¼‰â†’ éœ€è¦è½¬æ¢ä¸ºLIBEROçš„[-1, 1]èŒƒå›´
    - æ—‹è½¬ï¼š6Dè¡¨ç¤ºï¼ˆä»å¼§åº¦è½¬æ¢ï¼‰â†’ éœ€è¦è½¬æ¢ä¸ºLIBEROçš„[-1, 1]èŒƒå›´
    - gripperï¼š[0, 1] èŒƒå›´ â†’ è½¬æ¢ä¸ºLIBEROçš„[-1, 1]èŒƒå›´
    """
    # RDTè¾“å‡º128ç»´åŠ¨ä½œï¼Œéœ€è¦ä»æ­£ç¡®çš„ç´¢å¼•æå–LIBEROåŠ¨ä½œ
    action_128d = rdt_action[0, 0, :].cpu().numpy()  # (128,)
    
    # === æ­¥éª¤1: æå–ä½ç½®ï¼ˆç‰©ç†å•ä½ï¼šç±³ï¼‰ ===
    pos_x_idx = STATE_VEC_IDX_MAPPING["right_eef_pos_x"]  # ç´¢å¼•30
    pos_y_idx = STATE_VEC_IDX_MAPPING["right_eef_pos_y"]  # ç´¢å¼•31
    pos_z_idx = STATE_VEC_IDX_MAPPING["right_eef_pos_z"]  # ç´¢å¼•32
    
    # RDTè¾“å‡ºçš„æ˜¯ç‰©ç†å•ä½ï¼ˆç±³ï¼‰
    pos_x_meters = action_128d[pos_x_idx]
    pos_y_meters = action_128d[pos_y_idx]
    pos_z_meters = action_128d[pos_z_idx]
    
    # è½¬æ¢ä¸ºLIBEROçš„å½’ä¸€åŒ–èŒƒå›´: ç±³ â†’ [-1, 1]
    # ä¿®æ­£ï¼šä½¿ç”¨å®é™…æµ‹é‡çš„ç¼©æ”¾å› å­ 0.012 è€Œä¸æ˜¯ 0.05
    # 
    # âš ï¸ åæ ‡ç³»ä¿®æ­£ï¼šRDTé¢„è®­ç»ƒæ•°æ®ä¸LIBEROåæ ‡ç³»ä¸åŒ
    # æ ¹æ®è§‚å¯Ÿã€Œä¸Šä¸‹åï¼Œå·¦å³åã€ï¼Œç¿»è½¬Xå’ŒZè½´
    pos_x_norm = -pos_x_meters / 0.012  # ç¿»è½¬Xè½´ï¼ˆå·¦å³ï¼‰
    pos_y_norm = pos_y_meters / 0.012   # Yè½´ä¸å˜ï¼ˆå‰åï¼‰
    pos_z_norm = -pos_z_meters / 0.012  # ç¿»è½¬Zè½´ï¼ˆä¸Šä¸‹ï¼‰
    
    # === æ­¥éª¤2: æå–6Dæ—‹è½¬å¹¶è½¬æ¢ä¸ºæ¬§æ‹‰è§’ï¼ˆå¼§åº¦ï¼‰ ===
    ori_indices = [
        STATE_VEC_IDX_MAPPING["right_eef_angle_0"],  # ç´¢å¼•33
        STATE_VEC_IDX_MAPPING["right_eef_angle_1"],  # ç´¢å¼•34
        STATE_VEC_IDX_MAPPING["right_eef_angle_2"],  # ç´¢å¼•35
        STATE_VEC_IDX_MAPPING["right_eef_angle_3"],  # ç´¢å¼•36
        STATE_VEC_IDX_MAPPING["right_eef_angle_4"],  # ç´¢å¼•37
        STATE_VEC_IDX_MAPPING["right_eef_angle_5"]   # ç´¢å¼•38
    ]
    
    ori_6d = np.array([action_128d[idx] for idx in ori_indices])
    
    # 6Dæ—‹è½¬è½¬æ¬§æ‹‰è§’ï¼ˆå¼§åº¦ï¼‰
    ori_euler_rad = convert_6d_rotation_to_euler(ori_6d)  # ç‰©ç†å•ä½ï¼šå¼§åº¦
    
    # === æ­¥éª¤3: è½¬æ¢ä¸ºLIBEROçš„å½’ä¸€åŒ–èŒƒå›´: å¼§åº¦ â†’ [-1, 1] ===
    # [-0.5, 0.5]å¼§åº¦ å¯¹åº” [-1, 1]
    ori_x_norm = ori_euler_rad[0] / 0.5
    ori_y_norm = ori_euler_rad[1] / 0.5
    ori_z_norm = ori_euler_rad[2] / 0.5
    
    # === æ­¥éª¤4: æå–Gripper ===
    gripper_idx = STATE_VEC_IDX_MAPPING["right_gripper_open"]  # ç´¢å¼•10
    gripper_01 = action_128d[gripper_idx]  # [0, 1]èŒƒå›´
    
    # å°†gripperä»[0, 1]æ˜ å°„åˆ°LIBEROçš„[-1, 1]èŒƒå›´
    gripper_norm = gripper_01 * 2.0 - 1.0
    
    # === æ­¥éª¤5: æ„å»ºLIBEROåŠ¨ä½œå‘é‡ ===
    # [pos_x, pos_y, pos_z, ori_x, ori_y, ori_z, gripper]
    # æ‰€æœ‰å€¼éƒ½åº”è¯¥åœ¨[-1, 1]èŒƒå›´å†…
    libero_action = np.array([
        pos_x_norm, pos_y_norm, pos_z_norm,
        ori_x_norm, ori_y_norm, ori_z_norm,
        gripper_norm
    ])
    
    # Clipåˆ°[-1, 1]èŒƒå›´ä»¥ç¡®ä¿å®‰å…¨
    # è½»å¾®çš„æ•°å€¼è¯¯å·®å¯èƒ½å¯¼è‡´è¶…å‡ºèŒƒå›´
    libero_action = np.clip(libero_action, -1.0, 1.0)
    
    return libero_action

def evaluate_rdt_on_libero(model: RDTLIBEROModel, 
                          benchmark_name: str = "libero_90",
                          num_tasks: int = 5,
                          max_steps: int = 100,
                          record_video: bool = False,
                          video_output_dir: str = "videos") -> Dict:
    """åœ¨LIBEROåŸºå‡†ä¸Šè¯„ä¼°RDTæ¨¡å‹"""
    
    print(f"ğŸš€ å¼€å§‹RDTåœ¨{benchmark_name}ä¸Šçš„è¯„ä¼°")
    
    # åˆ›å»ºè§†é¢‘è¾“å‡ºç›®å½•
    if record_video:
        os.makedirs(video_output_dir, exist_ok=True)
        print(f"ğŸ¥ è§†é¢‘å½•åˆ¶å·²å¯ç”¨ï¼Œè¾“å‡ºç›®å½•: {video_output_dir}")
    
    # è®¾ç½®LIBEROç¯å¢ƒ
    libero_path = "/home/ubuntu/LIBERO/libero/libero"
    libero.set_libero_default_path(libero_path)
    
    # è·å–åŸºå‡†
    benchmark_dict = benchmark.get_benchmark_dict()
    libero_benchmark = benchmark_dict[benchmark_name]()
    
    results = {
        "total_tasks": min(num_tasks, len(libero_benchmark.get_task_names())),
        "successful_tasks": 0,
        "total_steps": 0,
        "task_results": []
    }
    
    # è¯„ä¼°æŒ‡å®šæ•°é‡çš„ä»»åŠ¡
    for task_idx in range(results["total_tasks"]):
        task_name = libero_benchmark.get_task_names()[task_idx]
        print(f"\nğŸ“‹ è¯„ä¼°ä»»åŠ¡ {task_idx+1}/{results['total_tasks']}: {task_name}")
        
        # åˆå§‹åŒ–è§†é¢‘å½•åˆ¶å™¨
        video_recorder = None
        if record_video:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            video_filename = f"task_{task_idx+1:02d}_{task_name[:30]}_{timestamp}.mp4"
            video_path = os.path.join(video_output_dir, video_filename)
            video_recorder = VideoRecorder(video_path, fps=10, width=128, height=128)
            print(f"  ğŸ¥ å¼€å§‹å½•åˆ¶è§†é¢‘: {video_filename}")
        
        try:
            # è·å–ä»»åŠ¡
            task = libero_benchmark.get_task(task_idx)
            
            # åˆ›å»ºç¯å¢ƒ
            bddl_files_path = libero.get_libero_path("bddl_files")
            bddl_file_path = os.path.join(bddl_files_path, task.problem_folder, task.bddl_file)
            
            env_args = {
                "bddl_file_name": bddl_file_path,
                "camera_heights": 128,
                "camera_widths": 128
            }
            
            env = OffScreenRenderEnv(**env_args)
            
            # é‡ç½®ç¯å¢ƒ
            obs = env.reset()
            
            # è®¾ç½®åˆå§‹çŠ¶æ€
            init_states = libero_benchmark.get_task_init_states(task_idx)
            if len(init_states) > 0:
                env.set_init_state(init_states[0])
            
            # ç¼–ç ä»»åŠ¡æè¿°
            text_embed = model.encode_instruction(task_name)
            print(f"  ğŸ“ ä»»åŠ¡æè¿°: {task_name}")
            print(f"  ğŸ“ æ–‡æœ¬åµŒå…¥å½¢çŠ¶: {text_embed.shape}")
            print(f"  ğŸ“ æ–‡æœ¬åµŒå…¥èŒƒå›´: [{text_embed.min():.3f}, {text_embed.max():.3f}]")
            
            # é‡ç½®æ¨¡å‹
            model.reset()
            
            # è¿è¡Œæ¨ç†
            task_success = False
            task_steps = 0
            
            print(f"  ğŸ¯ å¼€å§‹æ¨ç†å¾ªç¯ï¼Œæœ€å¤§æ­¥æ•°: {max_steps}")
            
            for step in range(max_steps):
                print(f"    ğŸ“ æ­¥éª¤ {step+1}:")
                
                # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿æ¯æ¬¡æ¨ç†éƒ½æœ‰ä¸åŒçš„éšæœºæ€§
                torch.manual_seed(task_idx * 1000 + step)
                np.random.seed(task_idx * 1000 + step)
                
                # å‡†å¤‡å›¾åƒè¾“å…¥
                img = obs["agentview_image"]
                images = [Image.fromarray(img), None, None]  # [cam_high, cam_right_wrist, cam_left_wrist]
                print(f"      ğŸ“· å›¾åƒå½¢çŠ¶: {img.shape}")
                
                # å½•åˆ¶è§†é¢‘å¸§
                if video_recorder is not None:
                    video_recorder.add_frame(img)
                
                # è½¬æ¢çŠ¶æ€
                rdt_state = convert_libero_state_to_rdt(obs)
                print(f"      ğŸ”§ RDTçŠ¶æ€å½¢çŠ¶: {rdt_state.shape}")
                print(f"      ğŸ”§ å…³èŠ‚ä½ç½®: {obs['robot0_joint_pos'][:3]}")
                print(f"      ğŸ”§ æœ«ç«¯æ‰§è¡Œå™¨ä½ç½®: {obs['robot0_eef_pos']}")
                
                # æ¨¡å‹æ¨ç†
                print(f"      ğŸ§  æ‰§è¡ŒRDTæ¨ç†...")
                rdt_actions = model.step(rdt_state, images, text_embed)
                print(f"      ğŸ§  RDTè¾“å‡ºå½¢çŠ¶: {rdt_actions.shape}")
                print(f"      ğŸ§  RDTè¾“å‡ºèŒƒå›´: [{rdt_actions.min():.3f}, {rdt_actions.max():.3f}]")
                
                # æ£€æŸ¥RDTè¾“å‡ºçš„å˜åŒ–
                if step == 0:
                    first_rdt_output = rdt_actions.clone()
                    # ä¿å­˜ç¬¬ä¸€ä¸ªä»»åŠ¡çš„çŠ¶æ€å’Œæ–‡æœ¬åµŒå…¥ç”¨äºæ¯”è¾ƒ
                    if task_idx == 0:
                        first_task_state = rdt_state.clone()
                        first_task_text = text_embed.clone()
                else:
                    diff = torch.abs(rdt_actions - first_rdt_output).mean()
                    print(f"      ğŸ” RDTè¾“å‡ºä¸ç¬¬ä¸€æ­¥çš„å·®å¼‚: {diff:.6f}")
                    
                    # å¦‚æœæ˜¯ç¬¬äºŒä¸ªä»»åŠ¡çš„ç¬¬ä¸€æ­¥ï¼Œæ¯”è¾ƒä¸ç¬¬ä¸€ä¸ªä»»åŠ¡çš„å·®å¼‚
                    if task_idx == 1 and step == 0:
                        state_diff = torch.abs(rdt_state - first_task_state).mean()
                        text_diff = torch.abs(text_embed - first_task_text).mean()
                        print(f"      ğŸ” ä¸ä»»åŠ¡1çš„çŠ¶æ€å·®å¼‚: {state_diff:.6f}")
                        print(f"      ğŸ” ä¸ä»»åŠ¡1çš„æ–‡æœ¬å·®å¼‚: {text_diff:.6f}")
                
                # è½¬æ¢åŠ¨ä½œ
                libero_action = convert_rdt_action_to_libero(rdt_actions)
                print(f"      âš¡ LIBEROåŠ¨ä½œ: {libero_action}")
                print(f"      âš¡ åŠ¨ä½œèŒƒå›´: [{libero_action.min():.3f}, {libero_action.max():.3f}]")
                
                # æ‰§è¡ŒåŠ¨ä½œ
                print(f"      ğŸ® æ‰§è¡ŒåŠ¨ä½œ...")
                obs, reward, done, info = env.step(libero_action)
                task_steps += 1
                
                print(f"      ğŸ“Š å¥–åŠ±: {reward:.3f}, å®Œæˆ: {done}")
                if 'success' in info:
                    print(f"      ğŸ¯ æˆåŠŸçŠ¶æ€: {info['success']}")
                
                if done:
                    task_success = info.get("success", False)
                    print(f"      ğŸ Episodeç»“æŸ: æˆåŠŸ={task_success}")
                    break
            
            # è®°å½•ç»“æœ
            task_result = {
                "task_name": task_name,
                "success": task_success,
                "steps": task_steps,
                "reward": reward
            }
            results["task_results"].append(task_result)
            
            if task_success:
                results["successful_tasks"] += 1
                print(f"  âœ… ä»»åŠ¡æˆåŠŸå®Œæˆï¼Œæ­¥æ•°: {task_steps}")
            else:
                print(f"  âŒ ä»»åŠ¡å¤±è´¥ï¼Œæ­¥æ•°: {task_steps}")
            
            # ä¿å­˜è§†é¢‘
            if video_recorder is not None:
                video_recorder.save_video()
            
            results["total_steps"] += task_steps
            
            env.close()
            
        except Exception as e:
            print(f"  âŒ ä»»åŠ¡æ‰§è¡Œå‡ºé”™: {e}")
            results["task_results"].append({
                "task_name": task_name,
                "success": False,
                "steps": 0,
                "reward": 0,
                "error": str(e)
            })
    
    # è®¡ç®—æˆåŠŸç‡
    results["success_rate"] = results["successful_tasks"] / results["total_tasks"]
    results["avg_steps"] = results["total_steps"] / results["total_tasks"]
    
    print(f"\nğŸ“Š è¯„ä¼°ç»“æœ:")
    print(f"  æ€»ä»»åŠ¡æ•°: {results['total_tasks']}")
    print(f"  æˆåŠŸä»»åŠ¡æ•°: {results['successful_tasks']}")
    print(f"  æˆåŠŸç‡: {results['success_rate']:.2%}")
    print(f"  å¹³å‡æ­¥æ•°: {results['avg_steps']:.1f}")
    
    return results

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="RDTåœ¨LIBEROä¸Šçš„æ¨ç†è¯„ä¼°")
    parser.add_argument("--config", type=str, default="configs/base.yaml",
                       help="RDTé…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--pretrained", type=str, default="checkpoints/rdt-1b",
                       help="é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„")
    parser.add_argument("--text_encoder", type=str, default="google/t5-v1_1-xxl",
                       help="æ–‡æœ¬ç¼–ç å™¨è·¯å¾„")
    parser.add_argument("--vision_encoder", type=str, default="google/siglip-so400m-patch14-384",
                       help="è§†è§‰ç¼–ç å™¨è·¯å¾„")
    parser.add_argument("--benchmark", type=str, default="libero_90",
                       help="LIBEROåŸºå‡†åç§°")
    parser.add_argument("--num_tasks", type=int, default=5,
                       help="è¯„ä¼°ä»»åŠ¡æ•°é‡")
    parser.add_argument("--max_steps", type=int, default=100,
                       help="æ¯ä¸ªä»»åŠ¡æœ€å¤§æ­¥æ•°")
    parser.add_argument("--record_video", action="store_true",
                       help="æ˜¯å¦å½•åˆ¶è§†é¢‘")
    parser.add_argument("--video_output_dir", type=str, default="videos",
                       help="è§†é¢‘è¾“å‡ºç›®å½•")
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–æ¨¡å‹
    print("ğŸ¤– åˆå§‹åŒ–RDTæ¨¡å‹...")
    model = RDTLIBEROModel(
        config_path=args.config,
        pretrained_path=args.pretrained,
        text_encoder_path=args.text_encoder,
        vision_encoder_path=args.vision_encoder
    )
    
    # è¿è¡Œè¯„ä¼°
    results = evaluate_rdt_on_libero(
        model=model,
        benchmark_name=args.benchmark,
        num_tasks=args.num_tasks,
        max_steps=args.max_steps,
        record_video=args.record_video,
        video_output_dir=args.video_output_dir
    )
    
    # ä¿å­˜ç»“æœ
    import json
    with open("libero_evaluation_results.json", "w") as f:
        json.dump(results, f, indent=2)
    
    print(f"\nğŸ’¾ è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: libero_evaluation_results.json")

if __name__ == "__main__":
    main()