#!/usr/bin/env python3
"""
RDTåœ¨LIBEROä»»åŠ¡ä¸Šçš„æ¨ç†è¯„ä¼°è„šæœ¬
åŸºäºRDTçš„æ¨ç†æµç¨‹ï¼Œé€‚é…LIBEROç¯å¢ƒ
"""

import sys
import os
import numpy as np
import torch
from PIL import Image
import yaml
import argparse
from typing import Dict, List, Tuple, Optional
import cv2
from datetime import datetime

# æ·»åŠ è·¯å¾„
script_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.abspath(os.path.dirname(script_dir))
libero_path1 = os.path.abspath(os.path.join(project_root, '..', 'LIBERO', 'libero'))
libero_path2 = os.path.abspath(os.path.join(project_root, '..', 'LIBERO', 'libero', 'libero'))

# å°†æ‰€æœ‰è·¯å¾„æ·»åŠ åˆ°sys.pathï¼ˆç¡®ä¿ä½¿ç”¨ç»å¯¹è·¯å¾„ï¼‰
for path in [libero_path1, libero_path2, project_root]:
    if path not in sys.path:
        sys.path.insert(0, path)

# å¯¼å…¥LIBEROæ¨¡å—
import libero
from libero import benchmark
from libero.envs import OffScreenRenderEnv

# å¯¼å…¥RDTæ¨¡å—
import importlib.util

# åŠ¨æ€å¯¼å…¥configs.state_vec
state_vec_path = os.path.join(project_root, "configs", "state_vec.py")
spec = importlib.util.spec_from_file_location("state_vec", state_vec_path)
state_vec_module = importlib.util.module_from_spec(spec)
sys.modules["state_vec"] = state_vec_module
spec.loader.exec_module(state_vec_module)
STATE_VEC_IDX_MAPPING = state_vec_module.STATE_VEC_IDX_MAPPING

from models.multimodal_encoder.siglip_encoder import SiglipVisionTower
from models.multimodal_encoder.t5_encoder import T5Embedder
from models.rdt_runner import RDTRunner
from utils.rotation_utils import convert_quaternion_to_6d_rotation, convert_6d_rotation_to_euler

class VideoRecorder:
    """è§†é¢‘å½•åˆ¶å™¨"""
    
    def __init__(self, output_path: str, fps: int = 30, width: int = 128, height: int = 128):
        """åˆå§‹åŒ–è§†é¢‘å½•åˆ¶å™¨"""
        self.output_path = output_path
        self.fps = fps
        self.width = width
        self.height = height
        self.frames = []
        
    def add_frame(self, image: np.ndarray):
        """æ·»åŠ å¸§"""
        if image is not None:
            # ç¡®ä¿å›¾åƒå°ºå¯¸æ­£ç¡®
            if image.shape[:2] != (self.height, self.width):
                image = cv2.resize(image, (self.width, self.height))
            self.frames.append(image)
    
    def save_video(self):
        """ä¿å­˜è§†é¢‘"""
        if not self.frames:
            print("âš ï¸ æ²¡æœ‰å¸§å¯ä»¥ä¿å­˜")
            return
            
        # åˆ›å»ºè§†é¢‘å†™å…¥å™¨
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(self.output_path, fourcc, self.fps, (self.width, self.height))
        
        for frame in self.frames:
            # ç¡®ä¿å¸§æ˜¯BGRæ ¼å¼
            if len(frame.shape) == 3 and frame.shape[2] == 3:
                # å‡è®¾æ˜¯RGBï¼Œè½¬æ¢ä¸ºBGR
                frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
            else:
                frame_bgr = frame
            out.write(frame_bgr)
        
        out.release()
        print(f"ğŸ¥ è§†é¢‘å·²ä¿å­˜åˆ°: {self.output_path}")
        
    def clear(self):
        """æ¸…ç©ºå¸§"""
        self.frames = []

class RDTLIBEROModel:
    """RDTæ¨¡å‹åœ¨LIBEROä¸Šçš„æ¨ç†åŒ…è£…å™¨"""
    
    def __init__(self, config_path: str, pretrained_path: str, 
                 text_encoder_path: str, vision_encoder_path: str):
        """åˆå§‹åŒ–RDTæ¨¡å‹"""
        self.config_path = config_path
        self.pretrained_path = pretrained_path
        self.text_encoder_path = text_encoder_path
        self.vision_encoder_path = vision_encoder_path
        
        # åŠ è½½é…ç½®
        with open(config_path, "r") as fp:
            self.config = yaml.safe_load(fp)
        
        # åˆå§‹åŒ–ç¼–ç å™¨
        self._init_encoders()
        
        # åˆå§‹åŒ–RDTæ¨¡å‹
        self._init_rdt_model()
        
    def _init_encoders(self):
        """åˆå§‹åŒ–æ–‡æœ¬å’Œè§†è§‰ç¼–ç å™¨"""
        # æ–‡æœ¬ç¼–ç å™¨
        self.text_encoder = T5Embedder(
            device="cuda",
            from_pretrained=self.text_encoder_path,
            cache_dir=None,
            model_max_length=77,
        )
        
        # è§†è§‰ç¼–ç å™¨
        self.vision_encoder = SiglipVisionTower(
            vision_tower=self.vision_encoder_path,
            args=self.config,
            delay_load=False,
        )
        
    def _init_rdt_model(self):
        """åˆå§‹åŒ–RDTæ¨¡å‹"""
        # è®¡ç®—å›¾åƒæ¡ä»¶é•¿åº¦
        # SigLIPçš„patchæ•°é‡è®¡ç®—: (image_size / patch_size)^2
        patch_size = self.vision_encoder.vision_tower.config.patch_size
        image_size = self.vision_encoder.vision_tower.config.image_size
        num_patches = (image_size // patch_size) ** 2
        
        img_cond_len = (self.config["common"]["img_history_size"] 
                       * self.config["common"]["num_cameras"] 
                       * num_patches)
        
        self.rdt_model = RDTRunner(
            action_dim=self.config["common"]["state_dim"],
            pred_horizon=self.config["common"]["action_chunk_size"],
            config=self.config["model"],
            lang_token_dim=self.config["model"]["lang_token_dim"],
            img_token_dim=self.config["model"]["img_token_dim"],
            state_token_dim=self.config["model"]["state_token_dim"],
            max_lang_cond_len=self.config["dataset"]["tokenizer_max_length"],
            img_cond_len=img_cond_len,
            img_pos_embed_config=[
                ("image", (self.config["common"]["img_history_size"], 
                    self.config["common"]["num_cameras"], 
                    -num_patches)),  
            ],
            lang_pos_embed_config=[
                ("lang", -self.config["dataset"]["tokenizer_max_length"]),
            ],
            dtype=torch.bfloat16,
        )
        
        # åŠ è½½é¢„è®­ç»ƒæƒé‡
        # æ”¯æŒä¸‰ç§æƒ…å†µï¼š
        # 1. ç›´æ¥ä¼ å…¥æ¨¡å‹æ–‡ä»¶è·¯å¾„ (å¦‚ .safetensors æˆ– .bin)
        # 2. ä¼ å…¥ç›®å½•ï¼ŒæŸ¥æ‰¾ pytorch_model.bin
        # 3. ä¼ å…¥ç›®å½•ï¼ŒæŸ¥æ‰¾ model.safetensors
        if os.path.isfile(self.pretrained_path):
            model_file = self.pretrained_path
        else:
            # ä¼˜å…ˆæŸ¥æ‰¾ pytorch_model.bin
            model_file = os.path.join(self.pretrained_path, "pytorch_model.bin")
            if not os.path.exists(model_file):
                # å¦‚æœæ²¡æœ‰ï¼ŒæŸ¥æ‰¾ model.safetensors
                model_file = os.path.join(self.pretrained_path, "model.safetensors")
        
        if os.path.exists(model_file):
            if model_file.endswith('.safetensors'):
                from safetensors.torch import load_file
                checkpoint = load_file(model_file)
                self.rdt_model.load_state_dict(checkpoint, strict=False)
                print(f"âœ… åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ (safetensors): {model_file}")
            else:
                checkpoint = torch.load(model_file, map_location="cpu")
                self.rdt_model.load_state_dict(checkpoint, strict=False)
                print(f"âœ… åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {model_file}")
        else:
            print(f"âš ï¸ é¢„è®­ç»ƒæ¨¡å‹ä¸å­˜åœ¨: {model_file}")
            
    def encode_instruction(self, instruction: str) -> torch.Tensor:
        """ç¼–ç ä»»åŠ¡æŒ‡ä»¤"""
        text_embeds, attention_mask = self.text_encoder.get_text_embeddings([instruction])
        return text_embeds
    
    def reset(self):
        """é‡ç½®æ¨¡å‹çŠ¶æ€"""
        device = "cuda"
        weight_dtype = torch.bfloat16
        self.rdt_model.eval()
        self.text_encoder.model.eval()
        self.vision_encoder.vision_tower.eval()
        
        self.rdt_model = self.rdt_model.to(device, dtype=weight_dtype)
        self.text_encoder.model = self.text_encoder.model.to(device, dtype=weight_dtype)
        self.vision_encoder.vision_tower = self.vision_encoder.vision_tower.to(device, dtype=weight_dtype)
        
    def step(self, state: torch.Tensor, images: List[Image.Image], 
             text_embed: torch.Tensor) -> torch.Tensor:
        """æ‰§è¡Œä¸€æ­¥æ¨ç†"""
        device = "cuda"
        dtype = torch.bfloat16
        
        # å¤„ç†å›¾åƒ - å‚è€ƒmaniskill_model.pyçš„å®ç°
        background_color = np.array([
            int(x*255) for x in self.vision_encoder.image_processor.image_mean
        ], dtype=np.uint8).reshape(1, 1, 3)
        background_image = np.ones((
            self.vision_encoder.image_processor.size["height"], 
            self.vision_encoder.image_processor.size["width"], 3), dtype=np.uint8
        ) * background_color
        
        image_tensor_list = []
        for image in images:
            if image is None:
                # ç”¨èƒŒæ™¯å›¾åƒæ›¿æ¢
                image = Image.fromarray(background_image)
            
            # é¢„å¤„ç†å›¾åƒ
            image = self.vision_encoder.image_processor.preprocess(image, return_tensors='pt')['pixel_values'][0]
            image_tensor_list.append(image)

        image_tensor = torch.stack(image_tensor_list, dim=0).to(device, dtype=dtype)
        
        # è·å–å›¾åƒç‰¹å¾ - å¤„ç†æ¯ä¸ªå›¾åƒ
        image_embeds_list = []
        for i in range(image_tensor.shape[0]):
            single_image = image_tensor[i:i+1]  # (1, 3, 384, 384)
            vision_output = self.vision_encoder.vision_tower(single_image)
            single_embeds = vision_output.last_hidden_state.detach()  # (1, 729, 1152)
            image_embeds_list.append(single_embeds)
        
        # æ‹¼æ¥æ‰€æœ‰å›¾åƒç‰¹å¾
        image_embeds = torch.cat(image_embeds_list, dim=1)  # (1, 729*3, 1152)
        
        # ä¸ºäº†åŒ¹é…è®­ç»ƒæ—¶çš„æ ¼å¼ï¼Œæˆ‘ä»¬éœ€è¦é‡å¤å›¾åƒç‰¹å¾ä»¥æ¨¡æ‹Ÿå†å²
        # è®­ç»ƒæ—¶ä½¿ç”¨ img_history_size=2ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦é‡å¤ä¸€æ¬¡
        img_history_size = self.config["common"]["img_history_size"]
        if img_history_size > 1:
            # é‡å¤å›¾åƒç‰¹å¾ä»¥æ¨¡æ‹Ÿå†å²
            image_embeds = image_embeds.repeat(1, img_history_size, 1)  # (1, 729*3*2, 1152)
        
        # é‡å¡‘ä¸ºæ­£ç¡®çš„å½¢çŠ¶: (batch_size, num_patches, hidden_size)
        image_embeds = image_embeds.reshape(1, -1, self.vision_encoder.vision_tower.config.hidden_size)
        
        # å¤„ç†çŠ¶æ€
        joints = state.to(device).unsqueeze(0)   # (1, 128)
        states = joints.to(device, dtype=dtype).unsqueeze(1)  # (1, 1, 128)
        
        # åˆ›å»ºçŠ¶æ€æ©ç 
        state_elem_mask = torch.ones(
            (1, 1, self.config["model"]["state_token_dim"]),
            device=device, dtype=dtype
        )
        
        
        ctrl_freqs = torch.tensor([20]).to(device)  # LIBEROæ§åˆ¶é¢‘ç‡
        
        text_embeds = text_embed.to(device, dtype=dtype)
        
        with torch.no_grad():
            trajectory = self.rdt_model.predict_action(
                lang_tokens=text_embeds,
                lang_attn_mask=torch.ones(
                    text_embeds.shape[:2], dtype=torch.bool,
                    device=text_embeds.device),
                img_tokens=image_embeds,
                state_tokens=states,
                action_mask=state_elem_mask,  
                ctrl_freqs=ctrl_freqs
            )
        
        return trajectory.to(torch.float32)

def convert_libero_state_to_rdt(obs: Dict, state_dim: int = 128) -> torch.Tensor:
    """å°†LIBEROè§‚å¯Ÿè½¬æ¢ä¸ºRDTçŠ¶æ€æ ¼å¼"""
    # æå–LIBEROçŠ¶æ€
    joint_pos = obs["robot0_joint_pos"]  # (7,)
    gripper_pos = obs["robot0_gripper_qpos"]  # (2,)
    eef_pos = obs["robot0_eef_pos"]  # (3,)
    eef_quat = obs["robot0_eef_quat"]  # (4,)
    
    # è®¡ç®—gripperçŠ¶æ€
    gripper_state = np.mean(gripper_pos)
    
    # ä½¿ç”¨ä¿®å¤åçš„å››å…ƒæ•°åˆ°6Dæ—‹è½¬è½¬æ¢å‡½æ•°
    eef_ori_6d = convert_quaternion_to_6d_rotation(eef_quat)
    
    # æ„å»º17ç»´LIBEROçŠ¶æ€å‘é‡
    libero_state = np.concatenate([
        joint_pos,           # 7ç»´
        [gripper_state],     # 1ç»´
        eef_pos,            # 3ç»´
        eef_ori_6d          # 6ç»´ (æ­£ç¡®çš„6Dæ—‹è½¬)
    ])  # æ€»å…±17ç»´
    
    # æ˜ å°„åˆ°RDTçš„128ç»´çŠ¶æ€ç©ºé—´
    rdt_state = np.zeros(state_dim)
    
    # ä½¿ç”¨å³è‡‚çš„ç´¢å¼•æ˜ å°„
    right_arm_indices = [
        STATE_VEC_IDX_MAPPING[f"right_arm_joint_{i}_pos"] for i in range(7)
    ] + [
        STATE_VEC_IDX_MAPPING["right_gripper_open"]
    ] + [
        STATE_VEC_IDX_MAPPING["right_eef_pos_x"],
        STATE_VEC_IDX_MAPPING["right_eef_pos_y"], 
        STATE_VEC_IDX_MAPPING["right_eef_pos_z"]
    ] + [
        STATE_VEC_IDX_MAPPING["right_eef_angle_0"],
        STATE_VEC_IDX_MAPPING["right_eef_angle_1"],
        STATE_VEC_IDX_MAPPING["right_eef_angle_2"],
        STATE_VEC_IDX_MAPPING["right_eef_angle_3"],
        STATE_VEC_IDX_MAPPING["right_eef_angle_4"],
        STATE_VEC_IDX_MAPPING["right_eef_angle_5"]
    ]
    
    # ç¡®ä¿ç´¢å¼•æ•°é‡åŒ¹é…
    min_len = min(len(libero_state), len(right_arm_indices))
    rdt_state[right_arm_indices[:min_len]] = libero_state[:min_len]
    
    # é‡è¦ï¼šè®­ç»ƒæ—¶stateæ²¡æœ‰å½’ä¸€åŒ–ï¼Œæ‰€ä»¥è¯„ä¼°æ—¶ä¹Ÿä¸èƒ½å½’ä¸€åŒ–ï¼
    # è®­ç»ƒä»£ç (train/dataset.py)ä¸­statesç›´æ¥ä½¿ç”¨åŸå§‹å€¼
    # å¦‚æœè¯„ä¼°æ—¶å½’ä¸€åŒ–ä¼šå¯¼è‡´è¾“å…¥åˆ†å¸ƒä¸åŒ¹é…ï¼Œé€ æˆé¢„æµ‹é”™è¯¯
    
    # ç›´æ¥è¿”å›åŸå§‹stateï¼Œä¸è¿›è¡Œå½’ä¸€åŒ–
    return torch.from_numpy(rdt_state).float()

# åœ¨æ¨¡å—çº§åˆ«åŠ è½½ç»Ÿè®¡ä¿¡æ¯å’Œå¯¼å…¥utilsï¼ˆé¿å…é‡å¤ï¼‰
import json
_LIBERO_STATS = None

def _get_libero_stats():
    """è·å–LIBEROæ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯ï¼ˆç¼“å­˜ï¼‰"""
    global _LIBERO_STATS
    if _LIBERO_STATS is None:
        dataset_stat_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'configs', 'dataset_stat.json')
        with open(dataset_stat_path, 'r') as f:
            stats = json.load(f)
        _LIBERO_STATS = stats['libero_90']
    return _LIBERO_STATS

def convert_rdt_action_to_libero(rdt_action: torch.Tensor, action_index: int = 0) -> np.ndarray:
    """
    å°†RDTåŠ¨ä½œï¼ˆç‰©ç†å•ä½ï¼‰è½¬æ¢ä¸ºLIBEROåŠ¨ä½œæ ¼å¼ï¼ˆå½’ä¸€åŒ–ï¼‰
    
    Args:
        rdt_action: RDTæ¨¡å‹è¾“å‡ºçš„actions (1, 64, 128)
        action_index: è¦è½¬æ¢çš„actionç´¢å¼• (0-63)
    
    é‡è¦ï¼šä¿®å¤åçš„RDTè®­ç»ƒä½¿ç”¨ç‰©ç†å•ä½ï¼ˆç¬¦åˆREADME IMPORTANT 3ï¼‰
    - ä½ç½®ï¼šå˜ç±³ï¼ˆç‰©ç†å•ä½ï¼Œè®­ç»ƒæ—¶*1.2ï¼‰â†’ éœ€è¦è½¬æ¢ä¸ºLIBEROçš„[-1, 1]èŒƒå›´
    - æ—‹è½¬ï¼š6Dè¡¨ç¤ºï¼ˆä»å¼§åº¦è½¬æ¢ï¼‰â†’ éœ€è¦è½¬æ¢ä¸ºLIBEROçš„[-1, 1]èŒƒå›´
    - gripperï¼š[0, 1] èŒƒå›´ â†’ è½¬æ¢ä¸ºLIBEROçš„[-1, 1]èŒƒå›´
    """
    # RDTè¾“å‡º128ç»´åŠ¨ä½œï¼Œéœ€è¦ä»æ­£ç¡®çš„ç´¢å¼•æå–LIBEROåŠ¨ä½œ
    action_128d = rdt_action[0, action_index, :].cpu().numpy()  # (128,)
    
    # === æ­¥éª¤1: æå–ä½ç½®ï¼ˆç‰©ç†å•ä½ï¼šå˜ç±³ï¼‰ ===
    pos_x_idx = STATE_VEC_IDX_MAPPING["right_eef_pos_x"]  # ç´¢å¼•30
    pos_y_idx = STATE_VEC_IDX_MAPPING["right_eef_pos_y"]  # ç´¢å¼•31
    pos_z_idx = STATE_VEC_IDX_MAPPING["right_eef_pos_z"]  # ç´¢å¼•32
    
    # RDTè¾“å‡ºçš„æ˜¯ç‰©ç†å•ä½ï¼ˆå˜ç±³ï¼‰
    # è®­ç»ƒæ—¶: pos_cm = pos_normalized * 1.2, èŒƒå›´ [-1.2cm, 1.2cm]
    pos_x_cm = action_128d[pos_x_idx]
    pos_y_cm = action_128d[pos_y_idx]
    pos_z_cm = action_128d[pos_z_idx]
    
    # è½¬æ¢ä¸ºLIBEROçš„å½’ä¸€åŒ–èŒƒå›´: å˜ç±³ â†’ [-1, 1]
    # æ­£ç¡®çš„è½¬æ¢ï¼šé™¤ä»¥è®­ç»ƒæ—¶çš„ç¼©æ”¾å› å­1.2
    pos_x_norm = pos_x_cm / 1.2
    pos_y_norm = pos_y_cm / 1.2
    pos_z_norm = pos_z_cm / 1.2
    
    # === æ­¥éª¤2: æå–6Dæ—‹è½¬å¹¶è½¬æ¢ä¸ºæ¬§æ‹‰è§’ï¼ˆå¼§åº¦ï¼‰ ===
    ori_indices = [
        STATE_VEC_IDX_MAPPING["right_eef_angle_0"],  # ç´¢å¼•33
        STATE_VEC_IDX_MAPPING["right_eef_angle_1"],  # ç´¢å¼•34
        STATE_VEC_IDX_MAPPING["right_eef_angle_2"],  # ç´¢å¼•35
        STATE_VEC_IDX_MAPPING["right_eef_angle_3"],  # ç´¢å¼•36
        STATE_VEC_IDX_MAPPING["right_eef_angle_4"],  # ç´¢å¼•37
        STATE_VEC_IDX_MAPPING["right_eef_angle_5"]   # ç´¢å¼•38
    ]
    
    ori_6d = np.array([action_128d[idx] for idx in ori_indices])
    
    # 6Dæ—‹è½¬è½¬æ¬§æ‹‰è§’ï¼ˆå¼§åº¦ï¼‰
    ori_euler_rad = convert_6d_rotation_to_euler(ori_6d)  # ç‰©ç†å•ä½ï¼šå¼§åº¦
    
    # === æ­¥éª¤3: è½¬æ¢ä¸ºLIBEROçš„å½’ä¸€åŒ–èŒƒå›´: å¼§åº¦ â†’ [-1, 1] ===
    # [-0.5, 0.5]å¼§åº¦ å¯¹åº” [-1, 1]
    ori_x_norm = ori_euler_rad[0] / 0.5
    ori_y_norm = ori_euler_rad[1] / 0.5
    ori_z_norm = ori_euler_rad[2] / 0.5
    
    # === æ­¥éª¤4: æå–Gripper ===
    gripper_idx = STATE_VEC_IDX_MAPPING["right_gripper_open"]  # ç´¢å¼•10
    gripper_01 = action_128d[gripper_idx]  # [0, 1]èŒƒå›´
    
    # å°†gripperä»[0, 1]æ˜ å°„åˆ°LIBEROçš„[-1, 1]èŒƒå›´
    gripper_norm = gripper_01 * 2.0 - 1.0
    
    # === æ­¥éª¤5: æ„å»ºLIBEROåŠ¨ä½œå‘é‡ ===
    # [pos_x, pos_y, pos_z, ori_x, ori_y, ori_z, gripper]
    # æ‰€æœ‰å€¼éƒ½åº”è¯¥åœ¨[-1, 1]èŒƒå›´å†…
    libero_action = np.array([
        pos_x_norm, pos_y_norm, pos_z_norm,
        ori_x_norm, ori_y_norm, ori_z_norm,
        gripper_norm
    ])
    
    # Clipåˆ°[-1, 1]èŒƒå›´ä»¥ç¡®ä¿å®‰å…¨
    # è½»å¾®çš„æ•°å€¼è¯¯å·®å¯èƒ½å¯¼è‡´è¶…å‡ºèŒƒå›´
    libero_action = np.clip(libero_action, -1.0, 1.0)
    
    return libero_action

def evaluate_rdt_on_libero(model: RDTLIBEROModel, 
                          benchmark_name: str = "libero_90",
                          num_tasks: int = 5,
                          max_steps: int = 100,
                          record_video: bool = False,
                          exec_horizon: int = 1,
                          video_output_dir: str = "videos") -> Dict:
    """åœ¨LIBEROåŸºå‡†ä¸Šè¯„ä¼°RDTæ¨¡å‹"""
    
    print(f"ğŸš€ å¼€å§‹RDTåœ¨{benchmark_name}ä¸Šçš„è¯„ä¼°")
    
    # åˆ›å»ºè§†é¢‘è¾“å‡ºç›®å½•
    if record_video:
        os.makedirs(video_output_dir, exist_ok=True)
        print(f"ğŸ¥ è§†é¢‘å½•åˆ¶å·²å¯ç”¨ï¼Œè¾“å‡ºç›®å½•: {video_output_dir}")
    
    # è®¾ç½®LIBEROç¯å¢ƒï¼ˆè‡ªåŠ¨æ£€æµ‹è·¯å¾„ï¼‰
    libero_path = os.environ.get("LIBERO_PATH")
    
    if not libero_path:
        # å°è¯•å‡ ä¸ªå¯èƒ½çš„è·¯å¾„
        possible_paths = [
            "../LIBERO/libero/libero",  # ä¸é¡¹ç›®åŒçº§
            "../../LIBERO/libero/libero",  # ä¸Šä¸¤çº§
            os.path.expanduser("~/LIBERO/libero/libero"),  # ç”¨æˆ·ç›®å½•
        ]
        for path in possible_paths:
            if os.path.exists(path):
                libero_path = path
                break
        
        if not libero_path:
            print("âš ï¸  è­¦å‘Šï¼šæœªæ‰¾åˆ°LIBEROè·¯å¾„ï¼Œä½¿ç”¨é»˜è®¤ç›¸å¯¹è·¯å¾„")
            libero_path = "../LIBERO/libero/libero"
    
    libero.set_libero_default_path(libero_path)
    print(f"ğŸ“ LIBEROè·¯å¾„: {libero_path}")
    
    # è·å–åŸºå‡†
    benchmark_dict = benchmark.get_benchmark_dict()
    libero_benchmark = benchmark_dict[benchmark_name]()
    
    results = {
        "total_tasks": min(num_tasks, len(libero_benchmark.get_task_names())),
        "successful_tasks": 0,
        "total_steps": 0,
        "task_results": []
    }
    
    # è¯„ä¼°æŒ‡å®šæ•°é‡çš„ä»»åŠ¡
    for task_idx in range(results["total_tasks"]):
        task_name = libero_benchmark.get_task_names()[task_idx]
        print(f"\nğŸ“‹ è¯„ä¼°ä»»åŠ¡ {task_idx+1}/{results['total_tasks']}: {task_name}")
        
        # åˆå§‹åŒ–è§†é¢‘å½•åˆ¶å™¨
        video_recorder = None
        if record_video:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            video_filename = f"task_{task_idx+1:02d}_{task_name[:30]}_{timestamp}.mp4"
            video_path = os.path.join(video_output_dir, video_filename)
            video_recorder = VideoRecorder(video_path, fps=10, width=128, height=128)
            print(f"  ğŸ¥ å¼€å§‹å½•åˆ¶è§†é¢‘: {video_filename}")
        
        try:
            # è·å–ä»»åŠ¡
            task = libero_benchmark.get_task(task_idx)
            
            # åˆ›å»ºç¯å¢ƒ
            bddl_files_path = libero.get_libero_path("bddl_files")
            bddl_file_path = os.path.join(bddl_files_path, task.problem_folder, task.bddl_file)
            
            env_args = {
                "bddl_file_name": bddl_file_path,
                "camera_heights": 128,
                "camera_widths": 128
            }
            
            env = OffScreenRenderEnv(**env_args)
            
            # é‡ç½®ç¯å¢ƒ
            obs = env.reset()
            
            # è®¾ç½®åˆå§‹çŠ¶æ€
            init_states = libero_benchmark.get_task_init_states(task_idx)
            if len(init_states) > 0:
                env.set_init_state(init_states[0])
            
            # ç¼–ç ä»»åŠ¡æè¿°
            print(f"\n  {'='*80}")
            print(f"  ğŸ¯ è¾“å…¥ç»™æ¨¡å‹çš„ä»»åŠ¡åç§°: {task_name}")
            print(f"  {'='*80}")
            text_embed = model.encode_instruction(task_name)
            print(f"  ğŸ“ æ–‡æœ¬åµŒå…¥å½¢çŠ¶: {text_embed.shape}")
            print(f"  ğŸ“ æ–‡æœ¬åµŒå…¥èŒƒå›´: [{text_embed.min():.3f}, {text_embed.max():.3f}]")
            
            # é‡ç½®æ¨¡å‹
            model.reset()
            
            # è¿è¡Œæ¨ç†
            task_success = False
            task_steps = 0
            done = False
            
            print(f"  ğŸ¯ å¼€å§‹æ¨ç†å¾ªç¯ï¼Œæœ€å¤§æ­¥æ•°: {max_steps}")
            
            for step in range(max_steps):
                if done:
                    print(f"    âš ï¸  Episodeå·²ç»ˆæ­¢ï¼Œåœæ­¢æ¨ç†")
                    break
                print(f"    ğŸ“ æ­¥éª¤ {step+1}:")
                
                # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿æ¯æ¬¡æ¨ç†éƒ½æœ‰ä¸åŒçš„éšæœºæ€§
                torch.manual_seed(task_idx * 1000 + step)
                np.random.seed(task_idx * 1000 + step)
                
                # å‡†å¤‡å›¾åƒè¾“å…¥ - ä½¿ç”¨è®­ç»ƒæ—¶ç›¸åŒçš„æ‘„åƒå¤´é…ç½®
                img_high = obs["agentview_image"]  # ç¬¬ä¸‰äººç§°è§†è§’
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ‰‹è…•ç›¸æœºï¼ˆeye_in_handï¼‰
                img_wrist = None
                if "robot0_eye_in_hand_image" in obs:
                    img_wrist = obs["robot0_eye_in_hand_image"]
                
                # æ„å»ºå›¾åƒåˆ—è¡¨ï¼š[cam_high, cam_right_wrist, cam_left_wrist]
                # ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼ˆè®­ç»ƒæ—¶LIBEROä½¿ç”¨agentview + eye_in_handï¼‰
                if img_wrist is not None:
                    images = [Image.fromarray(img_high), Image.fromarray(img_wrist), None]
                    if step == 0:
                        print(f"      ğŸ“· ä½¿ç”¨2ä¸ªæ‘„åƒå¤´: agentview + eye_in_hand")
                        print(f"      ğŸ“· agentviewå½¢çŠ¶: {img_high.shape}")
                        print(f"      ğŸ“· eye_in_handå½¢çŠ¶: {img_wrist.shape}")
                else:
                    images = [Image.fromarray(img_high), None, None]
                    if step == 0:
                        print(f"      ğŸ“· ä½¿ç”¨1ä¸ªæ‘„åƒå¤´: agentview only")
                        print(f"      ğŸ“· å›¾åƒå½¢çŠ¶: {img_high.shape}")
                
                # å½•åˆ¶è§†é¢‘å¸§ï¼ˆä½¿ç”¨ä¸»è§†è§’agentviewï¼‰
                if video_recorder is not None:
                    video_recorder.add_frame(img_high)
                
                # è½¬æ¢çŠ¶æ€
                rdt_state = convert_libero_state_to_rdt(obs)
                print(f"      ğŸ”§ RDTçŠ¶æ€å½¢çŠ¶: {rdt_state.shape}")
                print(f"      ğŸ”§ å…³èŠ‚ä½ç½®: {obs['robot0_joint_pos'][:3]}")
                print(f"      ğŸ”§ æœ«ç«¯æ‰§è¡Œå™¨ä½ç½®: {obs['robot0_eef_pos']}")
                
                # æ¨¡å‹æ¨ç†
                print(f"      ğŸ§  æ‰§è¡ŒRDTæ¨ç†...")
                rdt_actions = model.step(rdt_state, images, text_embed)
                print(f"      ğŸ§  RDTè¾“å‡ºå½¢çŠ¶: {rdt_actions.shape}")
                print(f"      ğŸ§  RDTè¾“å‡ºèŒƒå›´: [{rdt_actions.min():.3f}, {rdt_actions.max():.3f}]")
                
                # æ£€æŸ¥RDTè¾“å‡ºçš„å˜åŒ–
                if step == 0:
                    first_rdt_output = rdt_actions.clone()
                    # ä¿å­˜ç¬¬ä¸€ä¸ªä»»åŠ¡çš„çŠ¶æ€å’Œæ–‡æœ¬åµŒå…¥ç”¨äºæ¯”è¾ƒ
                    if task_idx == 0:
                        first_task_state = rdt_state.clone()
                        first_task_text = text_embed.clone()
                else:
                    diff = torch.abs(rdt_actions - first_rdt_output).mean()
                    print(f"      ğŸ” RDTè¾“å‡ºä¸ç¬¬ä¸€æ­¥çš„å·®å¼‚: {diff:.6f}")
                    
                    # å¦‚æœæ˜¯ç¬¬äºŒä¸ªä»»åŠ¡çš„ç¬¬ä¸€æ­¥ï¼Œæ¯”è¾ƒä¸ç¬¬ä¸€ä¸ªä»»åŠ¡çš„å·®å¼‚
                    if task_idx == 1 and step == 0:
                        state_diff = torch.abs(rdt_state - first_task_state).mean()
                        text_diff = torch.abs(text_embed - first_task_text).mean()
                        print(f"      ğŸ” ä¸ä»»åŠ¡1çš„çŠ¶æ€å·®å¼‚: {state_diff:.6f}")
                        print(f"      ğŸ” ä¸ä»»åŠ¡1çš„æ–‡æœ¬å·®å¼‚: {text_diff:.6f}")
                
                # æ‰§è¡Œå¤šæ­¥åŠ¨ä½œ (æ ¹æ®exec_horizon)
                print(f"      ğŸ® æ‰§è¡Œ {exec_horizon} æ­¥åŠ¨ä½œ...")
                for action_idx in range(exec_horizon):
                    if action_idx >= rdt_actions.shape[1]:  # è¶…å‡ºé¢„æµ‹èŒƒå›´
                        break
                    
                    # è½¬æ¢ç¬¬action_idxæ­¥çš„åŠ¨ä½œ
                    libero_action = convert_rdt_action_to_libero(rdt_actions, action_index=action_idx)
                    
                    if action_idx == 0:  # åªæ‰“å°ç¬¬ä¸€æ­¥çš„è¯¦ç»†ä¿¡æ¯
                        print(f"      âš¡ LIBEROåŠ¨ä½œ[0]: {libero_action}")
                        print(f"      âš¡ åŠ¨ä½œèŒƒå›´: [{libero_action.min():.3f}, {libero_action.max():.3f}]")
                    
                    # æ‰§è¡ŒåŠ¨ä½œ
                    obs, reward, done, info = env.step(libero_action)
                    task_steps += 1
                    
                    # å½•åˆ¶è§†é¢‘å¸§ï¼ˆä½¿ç”¨ä¸»è§†è§’agentviewï¼‰
                    if video_recorder is not None and action_idx > 0:
                        video_recorder.add_frame(obs["agentview_image"])
                    
                    # æ ¹æ®LIBEROæ ‡å‡†ï¼šdone=True è¡¨ç¤ºä»»åŠ¡æˆåŠŸ
                    if done:
                        task_success = done  # ä¿®å¤ï¼šç›´æ¥ä½¿ç”¨doneä½œä¸ºæˆåŠŸæ ‡å¿—
                        print(f"      âœ… Episodeåœ¨ç¬¬{action_idx+1}æ­¥ç»“æŸ: æˆåŠŸ={task_success}")
                        break
                
                if done:
                    break
                
                print(f"      ğŸ“Š å¥–åŠ±: {reward:.3f}, å®Œæˆ: {done}")
            
            # æœ€ç»ˆæ£€æŸ¥ï¼šå¦‚æœepisodeç»“æŸä½†è¿˜æ²¡æ ‡è®°æˆåŠŸï¼Œæ£€æŸ¥æœ€ç»ˆçŠ¶æ€
            if not task_success and not done:
                # æ£€æŸ¥æœ€ç»ˆçŠ¶æ€æ˜¯å¦æˆåŠŸ
                final_success = env.check_success()
                if final_success:
                    task_success = True
                    print(f"  â­ æœ€ç»ˆçŠ¶æ€æ£€æŸ¥: ä»»åŠ¡å·²æˆåŠŸ!")
            
            # è®°å½•ç»“æœ
            task_result = {
                "task_name": task_name,
                "success": task_success,
                "steps": task_steps,
                "reward": reward
            }
            results["task_results"].append(task_result)
            
            if task_success:
                results["successful_tasks"] += 1
                print(f"  âœ… ä»»åŠ¡æˆåŠŸå®Œæˆï¼Œæ­¥æ•°: {task_steps}")
            else:
                print(f"  âŒ ä»»åŠ¡å¤±è´¥ï¼Œæ­¥æ•°: {task_steps}")
            
            # ä¿å­˜è§†é¢‘
            if video_recorder is not None:
                video_recorder.save_video()
            
            results["total_steps"] += task_steps
            
            env.close()
            
        except Exception as e:
            print(f"  âŒ ä»»åŠ¡æ‰§è¡Œå‡ºé”™: {e}")
            
            # ç¡®ä¿å³ä½¿å‡ºé”™ä¹Ÿä¿å­˜è§†é¢‘
            if video_recorder is not None:
                try:
                    video_recorder.save_video()
                    print(f"  ğŸ’¾ è§†é¢‘å·²ä¿å­˜ï¼ˆä»»åŠ¡å¤±è´¥ï¼‰")
                except Exception as ve:
                    print(f"  âš ï¸  è§†é¢‘ä¿å­˜å¤±è´¥: {ve}")
            
            results["task_results"].append({
                "task_name": task_name,
                "success": False,
                "steps": task_steps if 'task_steps' in locals() else 0,
                "reward": 0,
                "error": str(e)
            })
    
    # è®¡ç®—æˆåŠŸç‡
    results["success_rate"] = results["successful_tasks"] / results["total_tasks"]
    results["avg_steps"] = results["total_steps"] / results["total_tasks"]
    
    print(f"\nğŸ“Š è¯„ä¼°ç»“æœ:")
    print(f"  æ€»ä»»åŠ¡æ•°: {results['total_tasks']}")
    print(f"  æˆåŠŸä»»åŠ¡æ•°: {results['successful_tasks']}")
    print(f"  æˆåŠŸç‡: {results['success_rate']:.2%}")
    print(f"  å¹³å‡æ­¥æ•°: {results['avg_steps']:.1f}")
    
    return results

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="RDTåœ¨LIBEROä¸Šçš„æ¨ç†è¯„ä¼°")
    parser.add_argument("--config", type=str, default="configs/base.yaml",
                       help="RDTé…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--pretrained", type=str, default="checkpoints/rdt-1b",
                       help="é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„")
    parser.add_argument("--text_encoder", type=str, default="google/t5-v1_1-xxl",
                       help="æ–‡æœ¬ç¼–ç å™¨è·¯å¾„")
    parser.add_argument("--vision_encoder", type=str, default="google/siglip-so400m-patch14-384",
                       help="è§†è§‰ç¼–ç å™¨è·¯å¾„")
    parser.add_argument("--benchmark", type=str, default="libero_90",
                       help="LIBEROåŸºå‡†åç§°")
    parser.add_argument("--num_tasks", type=int, default=5,
                       help="è¯„ä¼°ä»»åŠ¡æ•°é‡")
    parser.add_argument("--max_steps", type=int, default=100,
                       help="æ¯ä¸ªä»»åŠ¡æœ€å¤§æ­¥æ•°")
    parser.add_argument("--exec_horizon", type=int, default=1,
                       help="æ¯æ¬¡é¢„æµ‹åæ‰§è¡Œå¤šå°‘æ­¥actions (1-64)")
    parser.add_argument("--record_video", action="store_true",
                       help="æ˜¯å¦å½•åˆ¶è§†é¢‘")
    parser.add_argument("--video_output_dir", type=str, default="videos",
                       help="è§†é¢‘è¾“å‡ºç›®å½•")
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–æ¨¡å‹
    print("ğŸ¤– åˆå§‹åŒ–RDTæ¨¡å‹...")
    model = RDTLIBEROModel(
        config_path=args.config,
        pretrained_path=args.pretrained,
        text_encoder_path=args.text_encoder,
        vision_encoder_path=args.vision_encoder
    )
    
    # è¿è¡Œè¯„ä¼°
    results = evaluate_rdt_on_libero(
        model=model,
        benchmark_name=args.benchmark,
        num_tasks=args.num_tasks,
        max_steps=args.max_steps,
        exec_horizon=args.exec_horizon,
        record_video=args.record_video,
        video_output_dir=args.video_output_dir
    )
    
    # ä¿å­˜ç»“æœ
    import json
    with open("libero_evaluation_results.json", "w") as f:
        json.dump(results, f, indent=2)
    
    print(f"\nğŸ’¾ è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: libero_evaluation_results.json")

if __name__ == "__main__":
    main()