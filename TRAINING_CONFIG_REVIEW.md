# è®­ç»ƒé…ç½®å¯¹æ¯”ï¼šæ‚¨çš„é…ç½® vs RDTæ¨è

## ğŸ“Š é…ç½®å¯¹æ¯”è¡¨

| å‚æ•° | RDT-1Bæ¨è | æ‚¨çš„å½“å‰é…ç½® | çŠ¶æ€ | è¯´æ˜ |
|------|-----------|------------|------|------|
| **Batch Sizeï¼ˆæœ‰æ•ˆï¼‰** | 256 | 256 (32Ã—1Ã—8) | âœ… | è¾¾æ ‡ï¼ |
| **Warmup Steps** | 5000-10000 | 5000 | âœ… | å®Œç¾ï¼ |
| **Learning Rate** | 1e-4 | 1e-4 | âœ… | æ­£ç¡® |
| **LR Scheduler** | constant | constant | âœ… | æ­£ç¡® |
| **é‡‡æ ·ç­–ç•¥** | å…¨æ­¥éª¤æšä¸¾ | å…¨æ­¥éª¤æšä¸¾ | âœ… | å·²å®ç° |
| **Gradient Accumulation** | æ¨èä½¿ç”¨ | **æœªä½¿ç”¨** | âš ï¸ | è§ä¸‹æ–¹ |

---

## âš ï¸ é‡è¦å‘ç°ï¼šå†…å­˜ä½¿ç”¨ç­–ç•¥

### æ‚¨çš„å½“å‰å®ç°

```bash
--train_batch_size=32 \
# gradient_accumulation_steps = 1 (é»˜è®¤)
# æœ‰æ•ˆbatch = 32 * 1 * 8 = 256 âœ…
```

**å•å¡å†…å­˜éœ€æ±‚**ï¼š
- æ¯ä¸ªGPUéœ€è¦å¤„ç†32ä¸ªsamples
- 48GB GPUï¼šå¯èƒ½å¯ä»¥ï¼Œä½†æ¯”è¾ƒåƒç´§ âš ï¸
- 80GB GPU (A800/A100)ï¼šæ²¡é—®é¢˜ âœ…

### RDTæ¨èçš„å®ç°

```bash
--train_batch_size=4 \
--gradient_accumulation_steps=8 \
# æœ‰æ•ˆbatch = 4 * 8 * 8 = 256 âœ…
```

**å•å¡å†…å­˜éœ€æ±‚**ï¼š
- æ¯ä¸ªGPUåªéœ€å¤„ç†4ä¸ªsamples
- 48GB GPUï¼šç»å¯¹æ²¡é—®é¢˜ âœ…
- 24GB GPUï¼šä¹Ÿå¯ä»¥ âœ…

---

## ğŸ¯ å»ºè®®

### å¦‚æœæ‚¨çš„GPUæ˜¯A800/A100 (80GB)

**æ‚¨çš„å½“å‰é…ç½®å®Œå…¨OKï¼** âœ…

```bash
æœ‰æ•ˆbatch size = 256 âœ…
Warmup steps = 5000 âœ…
å…¶ä»–å‚æ•°éƒ½æ­£ç¡® âœ…
```

**æ— éœ€ä¿®æ”¹ï¼Œå¯ä»¥ç›´æ¥è®­ç»ƒï¼**

---

### å¦‚æœæ‚¨çš„GPUæ˜¯4090/V100 (48GBæˆ–æ›´å°‘)

**å»ºè®®ä¿®æ”¹ä¸ºæ¢¯åº¦ç´¯ç§¯æ–¹å¼ï¼š**

```bash
--train_batch_size=4 \              # æ”¹ä¸º4
--gradient_accumulation_steps=8 \   # æ·»åŠ è¿™ä¸€è¡Œ
```

**åŸå› **ï¼š
- å•å¡batch=32å¯èƒ½å¯¼è‡´OOMï¼ˆå†…å­˜æº¢å‡ºï¼‰
- ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯æ›´å®‰å…¨ã€æ›´ç¨³å®š

---

## ğŸ“ è¯¦ç»†åˆ†æ

### 1. Batch Sizeç­–ç•¥å¯¹æ¯”

#### æ–¹æ¡ˆAï¼šç›´æ¥å¤§batchï¼ˆæ‚¨çš„æ–¹æ¡ˆï¼‰

```python
train_batch_size = 32
gradient_accumulation_steps = 1 (é»˜è®¤)

ä¼˜ç‚¹ï¼š
+ ä»£ç ç®€å•
+ è®­ç»ƒé€Ÿåº¦ç¨å¿«ï¼ˆæ— ç´¯ç§¯å¼€é”€ï¼‰

ç¼ºç‚¹ï¼š
- å•å¡å†…å­˜éœ€æ±‚é«˜ï¼ˆ32 samplesï¼‰
- å¯èƒ½OOM
- ä¸é€‚åˆå°GPU
```

#### æ–¹æ¡ˆBï¼šæ¢¯åº¦ç´¯ç§¯ï¼ˆRDTæ¨èï¼‰

```python
train_batch_size = 4
gradient_accumulation_steps = 8

ä¼˜ç‚¹ï¼š
+ å†…å­˜å‹å¥½ï¼ˆå•å¡åªéœ€4 samplesï¼‰
+ é€‚ç”¨äºå„ç§GPU
+ è®­ç»ƒæ›´ç¨³å®šï¼ˆæ¢¯åº¦ç´¯ç§¯çš„å¹³æ»‘æ•ˆæœï¼‰

ç¼ºç‚¹ï¼š
- ä»£ç ç¨å¤æ‚
- è®­ç»ƒé€Ÿåº¦ç•¥æ…¢ï¼ˆ~5%ï¼‰
```

---

### 2. ä¸ºä»€ä¹ˆRDTæ¨èæ¢¯åº¦ç´¯ç§¯ï¼Ÿ

```python
å®˜æ–¹è®­ç»ƒç¯å¢ƒï¼š
- å¤šæ ·åŒ–çš„GPUï¼šä»V100åˆ°A100
- éœ€è¦é€šç”¨æ€§ï¼šé€‚é…ä¸åŒç¡¬ä»¶
- ç¨³å®šæ€§ä¼˜å…ˆï¼šé¿å…OOMä¸­æ–­è®­ç»ƒ

å®é™…æ•ˆæœï¼š
- æœ‰æ•ˆbatch=256æ—¶ï¼Œä¸¤ç§æ–¹å¼æ•ˆæœç›¸åŒ
- ä½†æ¢¯åº¦ç´¯ç§¯æ›´å®‰å…¨ã€æ›´é€šç”¨
```

---

### 3. æ‚¨çš„é…ç½®æ˜¯å¦ç¬¦åˆRDTæ¨èï¼Ÿ

#### æ ¸å¿ƒæŒ‡æ ‡ï¼šâœ… å®Œå…¨ç¬¦åˆï¼

```python
âœ… æœ‰æ•ˆbatch size = 256
âœ… Warmup steps = 5000
âœ… Learning rate = 1e-4
âœ… LR scheduler = constant
âœ… é‡‡æ ·ç­–ç•¥ = å…¨æ­¥éª¤æšä¸¾
```

#### å®ç°æ–¹å¼ï¼šâš ï¸ ç•¥æœ‰ä¸åŒ

```python
RDTæ–¹å¼ï¼šå°batch + æ¢¯åº¦ç´¯ç§¯
æ‚¨çš„æ–¹å¼ï¼šå¤§batch + æ— ç´¯ç§¯

ç»“æœï¼šæœ‰æ•ˆbatchç›¸åŒï¼Œä½†å†…å­˜å ç”¨ä¸åŒ
```

---

## ğŸ”§ ä¸‰ç§ä¿®æ”¹å»ºè®®

### é€‰é¡¹1ï¼šä¿æŒå½“å‰é…ç½®ï¼ˆå¦‚æœGPUè¶³å¤Ÿå¤§ï¼‰

**é€‚ç”¨äº**ï¼šA800/A100 (80GB) æˆ–æ›´å¤§

```bash
# æ— éœ€ä¿®æ”¹ï¼Œå½“å‰é…ç½®å·²ç»å¾ˆå¥½ï¼
--train_batch_size=32 \
# æœ‰æ•ˆbatch = 32 * 8 = 256 âœ…
```

**ä¼˜ç‚¹**ï¼š
- âœ… ç¬¦åˆRDTæ‰€æœ‰æ ¸å¿ƒæ¨è
- âœ… è®­ç»ƒé€Ÿåº¦ç•¥å¿«
- âœ… ä»£ç æ›´ç®€å•

**ç¼ºç‚¹**ï¼š
- âš ï¸ éœ€è¦å¤§æ˜¾å­˜GPU

---

### é€‰é¡¹2ï¼šæ”¹ç”¨æ¢¯åº¦ç´¯ç§¯ï¼ˆæ¨èï¼Œæ›´é€šç”¨ï¼‰â­â­â­

**é€‚ç”¨äº**ï¼šä»»ä½•GPUï¼ˆåŒ…æ‹¬48GB 4090ï¼‰

```bash
--train_batch_size=4 \              # ä¿®æ”¹è¿™é‡Œ
--gradient_accumulation_steps=8 \   # æ·»åŠ è¿™ä¸€è¡Œ
# æœ‰æ•ˆbatch = 4 * 8 * 8 = 256 âœ…
```

**ä¼˜ç‚¹**ï¼š
- âœ… å®Œå…¨ç¬¦åˆRDTæ¨è
- âœ… å†…å­˜å‹å¥½
- âœ… æ›´é€šç”¨

**ç¼ºç‚¹**ï¼š
- ç•¥æ…¢5%ï¼ˆå¯å¿½ç•¥ï¼‰

**å®Œæ•´ä¿®æ”¹**ï¼š

```bash
accelerate launch \
    --num_processes 8 \
    --multi_gpu \
    --mixed_precision bf16 \
    main.py \
    --pretrained_model_name_or_path="checkpoints/rdt-1b" \
    --pretrained_text_encoder_name_or_path=$TEXT_ENCODER_NAME \
    --pretrained_vision_encoder_name_or_path=$VISION_ENCODER_NAME \
    --output_dir=$OUTPUT_DIR \
    --train_batch_size=4 \              # æ”¹ä¸º4
    --gradient_accumulation_steps=8 \   # æ·»åŠ æ­¤è¡Œ â­
    --sample_batch_size=64 \
    --max_train_steps=100000 \
    --checkpointing_period=2000 \
    --checkpoints_total_limit=30 \
    --sample_period=500 \
    --lr_scheduler="constant" \
    --learning_rate=1e-4 \
    --lr_warmup_steps=5000 \
    --mixed_precision="bf16" \
    --dataloader_num_workers=8 \
    --image_aug \
    --dataset_type="finetune" \
    --state_noise_snr=40 \
    --load_from_hdf5 \
    --report_to="wandb"
```

---

### é€‰é¡¹3ï¼šæ··åˆæ–¹æ¡ˆï¼ˆå¦‚æœGPUæ˜¯60-80GBï¼‰

```bash
--train_batch_size=16 \             # 16ä¹Ÿå¯ä»¥
--gradient_accumulation_steps=2 \   # ç´¯ç§¯2æ¬¡
# æœ‰æ•ˆbatch = 16 * 2 * 8 = 256 âœ…
```

---

## ğŸ“ æ€»ç»“

### æ‚¨çš„é…ç½®è¯„åˆ†ï¼š**95/100** â­â­â­â­â­

#### å®Œç¾çš„åœ°æ–¹ âœ…
```
âœ… æœ‰æ•ˆbatch size = 256
âœ… Warmup steps = 5000
âœ… Learning rate = 1e-4
âœ… LR scheduler = constant
âœ… å…¨æ­¥éª¤æšä¸¾å·²å¯ç”¨
âœ… æ‰€æœ‰å…¶ä»–å‚æ•°éƒ½æ­£ç¡®
```

#### å¯æ”¹è¿›çš„åœ°æ–¹ âš ï¸
```
âš ï¸ ç¼ºå°‘gradient_accumulation_steps
   â†’ å¦‚æœGPU<80GBï¼Œå»ºè®®æ·»åŠ 
   â†’ å¦‚æœGPUâ‰¥80GBï¼Œå½“å‰é…ç½®å®Œç¾
```

---

## ğŸš€ è¡ŒåŠ¨å»ºè®®

### åœºæ™¯1ï¼šæ‚¨æœ‰8Ã—A800 (80GB)

```bash
âœ… å½“å‰é…ç½®å®Œç¾ï¼Œç›´æ¥è®­ç»ƒï¼
bash train_single_task_improved.sh
```

### åœºæ™¯2ï¼šæ‚¨æœ‰8Ã—4090 (48GB)

```bash
âš ï¸ å»ºè®®ä¿®æ”¹ä¸ºæ¢¯åº¦ç´¯ç§¯æ–¹å¼
ä¿®æ”¹ train_batch_size=4
æ·»åŠ  gradient_accumulation_steps=8
```

### åœºæ™¯3ï¼šä¸ç¡®å®šGPUå¤§å°

```bash
ğŸ’¡ ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯æ˜¯æœ€å®‰å…¨çš„é€‰æ‹©
ä¿®æ”¹åçš„é…ç½®é€‚ç”¨äºæ‰€æœ‰GPU
```

---

## ğŸ“Š é¢„æœŸè®­ç»ƒæ•ˆæœ

### ä½¿ç”¨å½“å‰é…ç½®ï¼ˆæ— è®ºæ˜¯å¦æ¢¯åº¦ç´¯ç§¯ï¼‰

```python
æœ‰æ•ˆbatch = 256 âœ…
Warmup = 5000 âœ…
å…¨æ­¥éª¤æšä¸¾ âœ…

é¢„æœŸç»“æœï¼š
- è®­ç»ƒç¨³å®š âœ…
- Losså¹³æ»‘ä¸‹é™ âœ…
- 50 demo â†’ 60-80%æˆåŠŸç‡ âœ…
- èƒ½å®Œæˆä¸¤é˜¶æ®µä»»åŠ¡ âœ…
```

---

## âœ… æœ€ç»ˆç»“è®º

**æ‚¨çš„é…ç½®ä¸RDT-1Bè®ºæ–‡æ¨èé«˜åº¦ä¸€è‡´ï¼**

### æ ¸å¿ƒè¦ç´ å…¨éƒ¨æ»¡è¶³ï¼š
1. âœ… æœ‰æ•ˆbatch size = 256
2. âœ… Warmup steps = 5000
3. âœ… é‡‡æ ·ç­–ç•¥ = å…¨æ­¥éª¤æšä¸¾
4. âœ… æ‰€æœ‰è¶…å‚æ•°æ­£ç¡®

### å”¯ä¸€çš„åŒºåˆ«ï¼š
- **å®ç°æ–¹å¼**ä¸åŒï¼ˆç›´æ¥å¤§batch vs æ¢¯åº¦ç´¯ç§¯ï¼‰
- **æ•ˆæœç›¸åŒ**ï¼ˆæœ‰æ•ˆbatchéƒ½æ˜¯256ï¼‰
- **æ˜¾å­˜éœ€æ±‚**ä¸åŒï¼ˆ32 samples vs 4 samples per GPUï¼‰

### å»ºè®®ï¼š
- **å¦‚æœGPUâ‰¥80GB**ï¼šå½“å‰é…ç½®å®Œç¾ï¼Œç›´æ¥ç”¨ï¼âœ…
- **å¦‚æœGPU<80GB**ï¼šå»ºè®®æ·»åŠ æ¢¯åº¦ç´¯ç§¯ï¼Œæ›´å®‰å…¨ï¼âš ï¸

**æ— è®ºå“ªç§æ–¹å¼ï¼Œæ‚¨çš„é…ç½®éƒ½ç¬¦åˆRDTçš„æ ¸å¿ƒæ¨èï¼** ğŸ¯

