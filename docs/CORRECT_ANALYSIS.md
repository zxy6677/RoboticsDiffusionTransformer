# 正确的问题分析

## 🎯 关键事实修正

### 实际情况
- ✅ **当前训练（50个demo）**：模型**无法学会抓碗**
- ✅ **之前训练（1个demo）**：模型**能过拟合，学会抓碗**

### 这说明了什么？

**问题不在于数据量！** 而在于：

1. **50个demo训练** → 模型泛化能力强，但可能过于"平均化"
   - 看到了50种不同的轨迹变化
   - 学到了"大致的模式"，但失去了精细控制
   - 在位置控制（关抽屉）上表现好
   - 在精细控制（gripper开合、抓碗）上表现差

2. **1个demo训练** → 模型过拟合，但学会了完整任务
   - 只看到1条固定轨迹
   - 完美记住了这条轨迹的每个细节
   - 包括gripper的精确开合时机
   - 能完成整个任务（关抽屉+抓碗）

---

## 🔍 真正的问题根源

### 问题1: exec_horizon=64 太大 ⭐⭐⭐⭐⭐（最可能的主要原因）

**分析**：
```
任务长度：213步
阶段转换点：第120步（关抽屉完成，该抓碗）

exec_horizon=64时的预测周期：
- Step 0:   预测 [0:64]，执行全部   (关抽屉初期)
- Step 64:  预测 [64:128]，执行全部 (关抽屉末期)  
- Step 128: 预测 [128:192]，执行全部 (应该抓碗)
            ↑
            问题：模型在step 128时，虽然抽屉已关，
            但之前的"惯性"导致它继续预测类似"关抽屉"的动作
```

**为什么1个demo训练能成功？**
- 因为模型完全记住了那条轨迹
- 在step 128时，它"知道"接下来该抓碗
- 即使exec_horizon=64，它也能预测出正确的抓碗动作序列

**为什么50个demo训练失败？**
- 50条轨迹在关抽屉阶段的变化很大
- 模型学到的是"关抽屉的平均模式"
- 到step 128时，它不确定接下来该做什么
- 在不确定的情况下，它倾向于继续之前的动作模式（惯性）

**解决方案**：
```bash
# 使用更小的exec_horizon，让模型更频繁地重新规划
--exec_horizon 16  # 推荐
--exec_horizon 8   # 更激进
```

---

### 问题2: 双摄像头不一致 ⭐⭐⭐

**训练时**：使用2个摄像头（agentview + eye_in_hand）
**评估时（修改前）**：只使用1个摄像头（agentview）

**为什么1个demo训练能成功？**
- 可能训练数据中第二个摄像头的信息不重要
- 或者模型已经过拟合到主摄像头

**为什么50个demo训练失败？**
- 50个demo中，手腕摄像头捕捉了更多变化
- 模型依赖手腕视角来判断抓取时机
- 评估时缺少这个信息，导致抓取失败

**解决方案**：
- 已修复：`eval_sim/eval_rdt_libero.py` 现在会使用2个摄像头

---

### 问题3: Loss权重问题 ⭐⭐

**分析**：
```
旋转维度的loss权重 ≈ 2000
位置+gripper的loss权重 ≈ 10

结果：
- 模型过度关注旋转精度
- 忽略gripper的开合（这是抓碗的关键）
```

**为什么1个demo训练能成功？**
- 虽然权重不平衡，但因为是过拟合
- 模型还是能记住gripper的开合时机

**为什么50个demo训练失败？**
- 50个demo中gripper的开合时机有微小差异
- 由于loss权重太小，模型没有充分学习这些差异
- 导致在新的情况下无法正确判断何时抓取

**解决方案**：
- 如果调整exec_horizon后仍失败，需要重新训练
- 调整loss权重，增加gripper的权重

---

## 🎯 正确的解决策略

### 优先级排序

#### 🥇 第一步：测试 exec_horizon=16（不需要重新训练）⭐⭐⭐⭐⭐

```bash
# 立即测试
bash test_dual_camera.sh  # exec_horizon=16 + 双摄像头

# 或者更激进
export CUDA_VISIBLE_DEVICES=1
python eval_sim/eval_rdt_libero.py \
  --config configs/base.yaml \
  --pretrained checkpoints/single_task_scene7/checkpoint-19000/ema/model.safetensors \
  --text_encoder google/t5-v1_1-xxl \
  --vision_encoder google/siglip-so400m-patch14-384 \
  --benchmark libero_90 \
  --num_tasks 2 \
  --max_steps 250 \
  --exec_horizon 8 \
  --record_video \
  --video_output_dir videos/dual_camera_horizon8
```

**预期结果**：
- ✅ 如果成功，说明问题就是exec_horizon太大
- ✅ 不需要重新训练
- ✅ 50个demo训练的模型其实已经学会了，只是需要更频繁的重新规划

---

#### 🥈 第二步：如果第一步失败，分析具体失败原因

观察视频和日志，判断：

**情况A**：抽屉关闭后，机器人根本不尝试抓碗
→ 说明阶段转换没有发生
→ 需要进一步减小exec_horizon（试试4或2）

**情况B**：机器人尝试抓碗，但gripper没有关闭
→ 说明loss权重问题
→ 需要重新训练，调整gripper的权重

**情况C**：机器人尝试抓碗，gripper关闭了，但位置不准
→ 说明需要第二个摄像头的信息
→ 检查是否真的使用了双摄像头

---

#### 🥉 第三步：如果需要重新训练

**方案A**：只训练"抓碗"阶段（课程学习）
```python
# 修改数据加载，只使用轨迹的后半部分（step 100-213）
# 让模型专注学习抓碗动作
```

**方案B**：调整loss权重
```python
# 修改 configs/dataset_stat.json
# 限制旋转维度的权重上限
action_std[33:39] = 0.1  # 旋转维度设置更大的std
```

**方案C**：用更小的action_chunk_size训练
```yaml
# configs/base.yaml
action_chunk_size: 32  # 从64改为32
```

---

## 📊 对比分析

### 1个demo训练 vs 50个demo训练

| 特性 | 1个demo | 50个demo |
|------|---------|----------|
| 数据量 | 少 | 多 |
| 泛化能力 | 差（过拟合） | 好 |
| 记忆能力 | 强（完全记住） | 弱（平均化） |
| 位置精度 | 可能偏差大 | 好 |
| Gripper控制 | 好（记住了） | 差（被平均化） |
| 任务完成率 | 高（在训练轨迹上） | 低（需要适应） |

### 核心矛盾

```
1个demo训练：
  优点：能完成任务（因为记住了完整流程）
  缺点：泛化能力差，换个场景就不行

50个demo训练：
  优点：泛化能力强，适应不同场景
  缺点：在当前评估设置下，无法完成任务转换
```

**关键洞察**：
> 50个demo训练的模型并不是"没学会抓碗"，
> 而是在 exec_horizon=64 的设置下，
> 无法在运行时进行阶段转换！

---

## 🎯 验证假设的实验

### 实验1：不同exec_horizon的对比

| exec_horizon | 预测次数 | 转换点覆盖 | 预期成功率 |
|-------------|---------|-----------|-----------|
| 64 (当前) | 3-4次 | ❌ 可能错过 | 0% |
| 32 | 7次 | ⚠️  可能覆盖 | 10-20% |
| 16 | 14次 | ✅ 很可能覆盖 | 30-50% |
| 8 | 27次 | ✅✅ 必定覆盖 | 40-60% |
| 4 | 53次 | ✅✅✅ 多次覆盖 | 50-70% |

### 实验2：1个demo训练 + 不同exec_horizon

**假设**：如果1个demo训练的模型在exec_horizon=64时能成功，
在exec_horizon=16时也应该能成功（甚至更好）

**如果这个假设成立**：
→ 说明50个demo训练的模型也应该能成功
→ 只需要调整exec_horizon

---

## 💡 最终建议

### 立即行动（今天就能测试）

1. **测试 exec_horizon=16**
   ```bash
   bash test_dual_camera.sh
   ```

2. **如果不行，测试 exec_horizon=8**
   ```bash
   # 修改test_dual_camera.sh中的参数
   --exec_horizon 8
   ```

3. **如果还不行，测试 exec_horizon=4**
   ```bash
   --exec_horizon 4
   ```

### 如果所有exec_horizon都不行

**那就确认了问题在于loss权重**，需要：
1. 创建新的数据集统计（增加gripper的权重）
2. 重新训练（预计30-40小时）

---

## 📝 结论

**之前的分析有误**：
- ❌ 问题不在于"只用了1个demo"
- ✅ 问题在于"exec_horizon=64太大"

**正确理解**：
- 50个demo训练的模型**已经学会了两个子任务**
- 只是在评估时，由于exec_horizon太大，**无法进行阶段转换**
- 解决方案：**使用更小的exec_horizon**（推荐16或8）

**为什么1个demo能成功**：
- 因为过拟合，模型"记住了"在step 128该做什么
- 即使exec_horizon=64，它也能预测出正确的转换

**为什么50个demo失败**：
- 虽然学会了两个子任务，但没有记住具体的转换时机
- 需要更频繁的观察（更小的exec_horizon）来判断何时转换

---

**核心结论**：先测试不同的exec_horizon，这很可能就能解决问题！🎯

