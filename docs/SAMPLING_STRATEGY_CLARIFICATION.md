# RDTé‡‡æ ·ç­–ç•¥ï¼šPretrain vs Finetune æ¾„æ¸…

## ğŸ¯ æ ¸å¿ƒç­”æ¡ˆ

**å…¨æ­¥éª¤æšä¸¾ç­–ç•¥ä¸»è¦ç”¨äºPretrainï¼ˆé¢„è®­ç»ƒï¼‰ï¼**

ä½†Finetuneæ—¶ï¼Œå®˜æ–¹è®¾è®¡**ä¹Ÿæ”¯æŒ**å…¨æ­¥éª¤æšä¸¾ï¼Œåªæ˜¯æˆ‘ä»¬çš„LIBEROå®ç°ç”¨äº†éšæœºé‡‡æ ·ã€‚

---

## ğŸ“Š ä»£ç æ¶æ„åˆ†æ

### train/dataset.py çš„åŒæ¨¡å¼è®¾è®¡

```python
class VLAConsumerDataset(Dataset):
    def __init__(
        self,
        dataset_type='pretrain',  # 'pretrain' æˆ– 'finetune'
        use_hdf5=False,           # æ˜¯å¦ä½¿ç”¨HDF5ç›´æ¥è¯»å–
        ...
    ):
        # æ¨¡å¼1: Pretrainï¼ˆé¢„è®­ç»ƒï¼‰
        if not use_hdf5:
            # ä½¿ç”¨buffer + producer/consumer
            self.buffer_dir = config["buf_path"]
            self.num_chunks = config["buf_num_chunks"]
            # æ•°æ®æ¥æºï¼šå…¨æ­¥éª¤æšä¸¾åçš„buffer
        
        # æ¨¡å¼2: Finetune with HDF5
        if use_hdf5:
            if dataset_type == 'finetune':
                # ä½¿ç”¨HDF5LIBERODatasetï¼ˆæˆ‘ä»¬å®ç°çš„ï¼‰
                self.hdf5_dataset = HDF5LIBERODataset(...)
            else:
                # ç†è®ºä¸Šä¹Ÿå¯ä»¥ç”¨HDF5åšpretrain
                self.hdf5_dataset = HDF5VLADataset()
    
    def __getitem__(self, index):
        if self.use_hdf5:
            # ä»HDF5ç›´æ¥è¯»å–ï¼ˆæˆ‘ä»¬çš„å®ç°ç”¨éšæœºé‡‡æ ·ï¼‰
            res = self.hdf5_dataset.get_item()
        else:
            # ä»bufferè¯»å–ï¼ˆé¢„å¤„ç†æ—¶å·²å…¨æ­¥éª¤æšä¸¾ï¼‰
            res = self._safe_load(index)
```

---

## ğŸ” ä¸¤ç§æ¨¡å¼è¯¦ç»†å¯¹æ¯”

### æ¨¡å¼1: Pretrainï¼ˆé¢„è®­ç»ƒï¼‰â†’ å…¨æ­¥éª¤æšä¸¾

#### å·¥ä½œæµç¨‹

```
1. ç¦»çº¿é¢„å¤„ç†ï¼ˆProducerï¼‰ï¼š
   data/episode_transform.py::flatten_episode()
   â†“
   for episode in dataset:
       for step in episode:  # å…¨æ­¥éª¤æšä¸¾ï¼
           sample = create_sample(step)
           save_to_buffer(sample)
   â†“
   Bufferå­˜å‚¨äº†æ‰€æœ‰stepsï¼ˆ100M-500M samplesï¼‰

2. åœ¨çº¿è®­ç»ƒï¼ˆConsumerï¼‰ï¼š
   train/dataset.py::VLAConsumerDataset
   â†“
   ä»bufferè¯»å–samplesï¼ˆå·²ç»æ˜¯å…¨æ­¥éª¤æšä¸¾çš„ç»“æœï¼‰
   â†“
   DataLoader shuffleè¿™äº›samples
```

#### ä»£ç è¯æ®

```python
# data/episode_transform.py ç¬¬224è¡Œ
def flatten_episode(episode: dict) -> tf.data.Dataset:
    step_data = []
    for i in range(tf.shape(states)[0]):  # éå†æ‰€æœ‰æ­¥éª¤ï¼
        step_data.append({
            'step_id': episode['step_id'][i],
            'state_chunk': past_states[i],
            'action_chunk': future_states[i],
            # ...
        })
    return tf.data.Dataset.from_tensor_slices(step_data)

# data/producer.py ç¬¬185è¡Œ
for episode_steps in vla_dataset:
    for step in episode_steps:  # æ¯ä¸ªstepéƒ½ä¿å­˜
        save_sample(step, chunk_dir, fill_chunk_item_idx)
```

#### ä½¿ç”¨åœºæ™¯

- âœ… å¤§è§„æ¨¡é¢„è®­ç»ƒ
- âœ… Open X-Embodimentæ•°æ®é›†
- âœ… 1M+ episodes
- âœ… éœ€è¦æœ€å¤§åŒ–æ•°æ®åˆ©ç”¨

---

### æ¨¡å¼2: Finetune with HDF5ï¼ˆæˆ‘ä»¬çš„å®ç°ï¼‰â†’ éšæœºé‡‡æ ·

#### å·¥ä½œæµç¨‹

```
1. ç›´æ¥ä»HDF5è¯»å–ï¼ˆæ— é¢„å¤„ç†ï¼‰ï¼š
   data/hdf5_libero_dataset.py::HDF5LIBERODataset
   â†“
   def get_item():
       file = random.choice(hdf5_files)  # éšæœºé€‰file
       episode = random.choice(episodes)  # éšæœºé€‰episode
       step_id = random.randint(0, len(episode))  # éšæœºé€‰step
       return sample
   â†“
   æ¯æ¬¡è°ƒç”¨è¿”å›1ä¸ªéšæœºsample

2. åœ¨çº¿è®­ç»ƒï¼š
   train/dataset.py::VLAConsumerDataset
   â†“
   æ¯æ¬¡__getitem__éƒ½è°ƒç”¨hdf5_dataset.get_item()
   â†“
   è¿”å›1ä¸ªéšæœºé‡‡æ ·çš„step
```

#### ä»£ç è¯æ®

```python
# data/hdf5_libero_dataset.py ç¬¬98è¡Œ
def get_item(self, index: int=None, state_only=False):
    """Get a training sample at a random timestep."""
    while True:
        if index is None:
            # éšæœºé€‰æ‹©episode
            file_path = np.random.choice(self.file_paths, 
                                        p=self.episode_sample_weights)
        
        # éšæœºé€‰æ‹©episodeä¸­çš„step
        episodes = list(f['data'].keys())
        episode_key = np.random.choice(episodes)  # éšæœºï¼
        
        # éšæœºé€‰æ‹©èµ·å§‹step
        step_id = np.random.randint(0, num_steps - self.CHUNK_SIZE)
        
        return sample
```

#### ä½¿ç”¨åœºæ™¯

- âœ… å°è§„æ¨¡å¾®è°ƒï¼ˆæˆ‘ä»¬çš„å®ç°ï¼‰
- âœ… LIBEROæ•°æ®é›†ï¼ˆ50 demosï¼‰
- âœ… ä¸éœ€è¦é¢„å¤„ç†
- âŒ æ•°æ®åˆ©ç”¨ç‡ä½ï¼ˆ~30%ï¼‰

---

## ğŸ¤” å®˜æ–¹Finetuneç­–ç•¥æ˜¯ä»€ä¹ˆï¼Ÿ

### å…³é”®é—®é¢˜ï¼šå®˜æ–¹å¾®è°ƒä¹Ÿç”¨å…¨æ­¥éª¤æšä¸¾å—ï¼Ÿ

**ç­”æ¡ˆï¼šå¾ˆå¯èƒ½æ˜¯çš„ï¼**

#### è¯æ®1: configs/base.yamlçš„é…ç½®

```yaml
dataset:
  buf_path: /path/to/buffer  # bufferè·¯å¾„
  buf_num_chunks: 512
  buf_chunk_size: 512
  epsd_len_thresh_low: 32
  epsd_len_thresh_high: 2048
```

è¿™äº›é…ç½®**ä¸åŒºåˆ†pretrainå’Œfinetune**ï¼Œè¯´æ˜ï¼š
- å®˜æ–¹è®¾è®¡çš„æ•°æ®pipelineæ˜¯é€šç”¨çš„
- Finetuneä¹Ÿå¯ä»¥ä½¿ç”¨buffer + å…¨æ­¥éª¤æšä¸¾

#### è¯æ®2: train/dataset.pyçš„æ¡ä»¶åˆ¤æ–­

```python
# ç¬¬128-136è¡Œ
if use_hdf5:
    if dataset_type == 'finetune':
        self.hdf5_dataset = HDF5LIBERODataset(...)  # æˆ‘ä»¬å®ç°çš„
    else:
        self.hdf5_dataset = HDF5VLADataset()  # å®˜æ–¹çš„ï¼ˆç©ºå®ç°ï¼‰
```

**å…³é”®ç‚¹**ï¼š
- `use_hdf5=True` åªæ˜¯ä¸€ä¸ª**å¯é€‰**æ¨¡å¼
- å®˜æ–¹å¯èƒ½åœ¨finetuneæ—¶ä»ç„¶ä½¿ç”¨ `use_hdf5=False` + bufferæ¨¡å¼
- æˆ‘ä»¬ä¸ºäº†æ–¹ä¾¿ï¼Œå®ç°äº†HDF5ç›´æ¥è¯»å–ï¼Œä½†ç”¨äº†éšæœºé‡‡æ ·

---

## ğŸ“ˆ å®˜æ–¹å¯èƒ½çš„Finetuneç­–ç•¥

### é€‰é¡¹A: Finetuneä¹Ÿç”¨å…¨æ­¥éª¤æšä¸¾ï¼ˆæ¨æµ‹ï¼‰

```bash
# å®˜æ–¹å¯èƒ½çš„finetuneæµç¨‹

# 1. é¢„å¤„ç†LIBEROæ•°æ®ï¼ˆç¦»çº¿ï¼‰
python data/producer.py \
  --dataset_type finetune \
  --n_workers 4 \
  --fill_up

# ç»“æœï¼š
# - 50 episodes Ã— 213 steps = 10,650 samples
# - å…¨éƒ¨ä¿å­˜åˆ°buffer
# - æ•°æ®åˆ©ç”¨ç‡100%

# 2. è®­ç»ƒï¼ˆåœ¨çº¿ï¼‰
python train/train.py \
  --dataset_type finetune \
  --use_hdf5 False  # ä½¿ç”¨bufferï¼
  # ...
```

**ä¼˜ç‚¹**ï¼š
- âœ… ä¸pretrainç­–ç•¥ä¸€è‡´
- âœ… æ•°æ®åˆ©ç”¨ç‡100%
- âœ… è®­ç»ƒç¨³å®š

**ç¼ºç‚¹**ï¼š
- âš ï¸ éœ€è¦é¢„å¤„ç†
- âš ï¸ éœ€è¦bufferå­˜å‚¨ç©ºé—´

---

### é€‰é¡¹B: æˆ‘ä»¬çš„å®ç°ï¼ˆHDF5 + éšæœºé‡‡æ ·ï¼‰

```bash
# æˆ‘ä»¬çš„finetuneæµç¨‹

# 1. æ— éœ€é¢„å¤„ç†ï¼Œç›´æ¥è®­ç»ƒ
python train/train.py \
  --dataset_type finetune \
  --use_hdf5 True \  # ç›´æ¥ä»HDF5è¯»å–
  --load_from_hdf5
  # ...
```

**ä¼˜ç‚¹**ï¼š
- âœ… ç®€å•ï¼Œæ— éœ€é¢„å¤„ç†
- âœ… çµæ´»

**ç¼ºç‚¹**ï¼š
- âŒ æ•°æ®åˆ©ç”¨ç‡ä½ï¼ˆ~30%ï¼‰
- âŒ å…³é”®æ—¶åˆ»é‡‡æ ·ä¸è¶³
- âŒ è®­ç»ƒä¸å¤Ÿç¨³å®š

---

## ğŸ¯ ç»“è®º

### å…¨æ­¥éª¤æšä¸¾çš„ä½¿ç”¨åœºæ™¯

| åœºæ™¯ | ä½¿ç”¨å…¨æ­¥éª¤æšä¸¾ï¼Ÿ | å®ç°æ–¹å¼ |
|------|---------------|---------|
| **Pretrain** | âœ… æ˜¯ï¼ˆç¡®å®šï¼‰ | Producer/Consumer + Buffer |
| **å®˜æ–¹Finetune** | âœ… å¾ˆå¯èƒ½ï¼ˆæ¨æµ‹ï¼‰ | Producer/Consumer + Buffer |
| **æˆ‘ä»¬çš„Finetune** | âŒ å¦ï¼ˆå®ç°é€‰æ‹©ï¼‰ | HDF5 + éšæœºé‡‡æ · |

### ä¸ºä»€ä¹ˆæˆ‘ä»¬æ²¡ç”¨å…¨æ­¥éª¤æšä¸¾ï¼Ÿ

1. **ç®€åŒ–å®ç°** - é¿å…å¤æ‚çš„é¢„å¤„ç†
2. **å¿«é€Ÿè¿­ä»£** - ç›´æ¥ä»HDF5è¯»å–ï¼Œæ–¹ä¾¿è°ƒè¯•
3. **å­˜å‚¨é™åˆ¶** - ä¸éœ€è¦400GB+ buffer
4. **ç†è§£ä¸è¶³** - æ²¡æ„è¯†åˆ°è¿™æ˜¯æ€§èƒ½å…³é”®

### è¿™æ˜¯é—®é¢˜å—ï¼Ÿ

**æ˜¯çš„ï¼Œè¿™æ˜¯ä¸€ä¸ªé‡è¦é—®é¢˜ï¼** â­â­â­â­â­

```
å½±å“ï¼š
1. æ•°æ®åˆ©ç”¨ç‡ï¼š100% â†’ 30%  (-70%)
2. å…³é”®æ—¶åˆ»è¦†ç›–ï¼š100% â†’ ~13%  (-87%)
3. è®­ç»ƒç¨³å®šæ€§ï¼šé«˜ â†’ ä¸­  (æ˜¾è‘—ä¸‹é™)

ç»“æœï¼š
50ä¸ªdemoè®­ç»ƒå¤±è´¥ï¼Œ1ä¸ªdemoåè€Œèƒ½è¿‡æ‹Ÿåˆ
```

---

## ğŸ’¡ æ”¹è¿›å»ºè®®

### çŸ­æœŸæ–¹æ¡ˆï¼ˆä¸æ”¹é‡‡æ ·ç­–ç•¥ï¼‰

```bash
# 1. å¢å¤§batch sizeï¼ˆå·²å®ç°ï¼‰
--gradient_accumulation_steps=8
# æœ‰æ•ˆbatch size: 256

# 2. è°ƒæ•´exec_horizonï¼ˆå·²å»ºè®®ï¼‰
--exec_horizon 16
```

### ä¸­æœŸæ–¹æ¡ˆï¼ˆæ”¹ä¸ºå…¨æ­¥éª¤æšä¸¾ï¼‰â­æ¨è

```python
# ä¿®æ”¹data/hdf5_libero_dataset.py
class HDF5LIBERODataset:
    def __init__(self, ...):
        # é¢„å¤„ç†ï¼šå±•å¹³æ‰€æœ‰episodes
        self.all_samples = []
        for hdf5_file in self.file_paths:
            with h5py.File(hdf5_file, 'r') as f:
                for episode_key in f['data'].keys():
                    episode = f['data'][episode_key]
                    num_steps = len(episode['actions'])
                    
                    # æ¯ä¸ªstepéƒ½åˆ›å»ºä¸€ä¸ªsample
                    for step_id in range(num_steps - CHUNK_SIZE):
                        self.all_samples.append({
                            'file': hdf5_file,
                            'episode': episode_key,
                            'step': step_id
                        })
        
        print(f"Total samples: {len(self.all_samples)}")
        # 50 demos Ã— ~150 valid steps = ~7,500 samples
    
    def __len__(self):
        return len(self.all_samples)  # 7,500
    
    def __getitem__(self, index):
        sample_info = self.all_samples[index]
        # åŠ è½½è¿™ä¸ªç‰¹å®šçš„sample
        # ...
```

**ä¼˜åŠ¿**ï¼š
1. âœ… æ•°æ®åˆ©ç”¨ç‡100%ï¼ˆvs å½“å‰30%ï¼‰
2. âœ… ä¸RDTå®˜æ–¹ç­–ç•¥ä¸€è‡´
3. âœ… è®­ç»ƒæ›´ç¨³å®š
4. âœ… å…³é”®æ—¶åˆ»100%è¦†ç›–

---

## ğŸ“Š æ€§èƒ½é¢„æœŸ

### å½“å‰é…ç½®ï¼ˆéšæœºé‡‡æ · + å°batchï¼‰

```
é‡‡æ ·ç­–ç•¥: éšæœºé‡‡æ ·
Batch size: 32
æ•°æ®åˆ©ç”¨ç‡: 30%
å…³é”®æ—¶åˆ»è¦†ç›–: 13%
â†’ é¢„æœŸæˆåŠŸç‡: 0-10% âŒ
```

### æ”¹è¿›æ–¹æ¡ˆ1ï¼ˆéšæœºé‡‡æ · + å¤§batchï¼‰

```
é‡‡æ ·ç­–ç•¥: éšæœºé‡‡æ ·
Batch size: 256  â† æ”¹è¿›
æ•°æ®åˆ©ç”¨ç‡: 30%
å…³é”®æ—¶åˆ»è¦†ç›–: 13%
â†’ é¢„æœŸæˆåŠŸç‡: 10-30% âš ï¸
```

### æ”¹è¿›æ–¹æ¡ˆ2ï¼ˆå…¨æ­¥éª¤æšä¸¾ + å¤§batchï¼‰

```
é‡‡æ ·ç­–ç•¥: å…¨æ­¥éª¤æšä¸¾  â† æ”¹è¿›
Batch size: 256  â† æ”¹è¿›
æ•°æ®åˆ©ç”¨ç‡: 100%  â† æ”¹è¿›
å…³é”®æ—¶åˆ»è¦†ç›–: 100%  â† æ”¹è¿›
â†’ é¢„æœŸæˆåŠŸç‡: 60-80% âœ…
```

---

## ğŸ“ æ ¸å¿ƒè¦ç‚¹

1. **å…¨æ­¥éª¤æšä¸¾ä¸»è¦ç”¨äºPretrain** âœ…
   - 100%ç¡®å®šï¼šå®˜æ–¹pretrainä½¿ç”¨å…¨æ­¥éª¤æšä¸¾

2. **å®˜æ–¹Finetuneå¾ˆå¯èƒ½ä¹Ÿç”¨å…¨æ­¥éª¤æšä¸¾** âœ…
   - ä»£ç è®¾è®¡æ”¯æŒ
   - configsä¸åŒºåˆ†pretrain/finetune
   - ç¬¦åˆæœ€å¤§åŒ–æ•°æ®åˆ©ç”¨çš„åŸåˆ™

3. **æˆ‘ä»¬çš„Finetuneç”¨äº†éšæœºé‡‡æ ·** âŒ
   - è¿™æ˜¯å®ç°é€‰æ‹©ï¼Œä¸æ˜¯å®˜æ–¹è®¾è®¡
   - å¯¼è‡´æ•°æ®åˆ©ç”¨ç‡ä½ã€è®­ç»ƒä¸ç¨³å®š
   - è¿™æ˜¯50demoè®­ç»ƒå¤±è´¥çš„å…³é”®åŸå› ä¹‹ä¸€

4. **å»ºè®®å°½å¿«æ”¹ä¸ºå…¨æ­¥éª¤æšä¸¾** â­â­â­â­â­
   - ä¸å®˜æ–¹ç­–ç•¥ä¸€è‡´
   - æœ€å¤§åŒ–æ•°æ®åˆ©ç”¨
   - æ˜¾è‘—æå‡æ€§èƒ½

---

**æ€»ç»“**ï¼šå…¨æ­¥éª¤æšä¸¾ä¸ä»…æ˜¯Pretrainçš„ç­–ç•¥ï¼Œä¹Ÿåº”è¯¥æ˜¯Finetuneçš„ç­–ç•¥ã€‚æˆ‘ä»¬çš„éšæœºé‡‡æ ·æ˜¯ä¸€ä¸ªç®€åŒ–å®ç°ï¼Œä½†ç‰ºç‰²äº†æ€§èƒ½ã€‚ğŸ¯

