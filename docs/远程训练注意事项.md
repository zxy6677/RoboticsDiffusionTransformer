# 远程训练注意事项 ⚠️

## 核心问题：是否需要重新计算统计信息？

### 答案：**视情况而定** 🎯

统计信息存储在 `configs/dataset_stat.json` 中，用于：
1. **Loss权重计算** - 不同维度的损失加权
2. **数据归一化** - 训练时的输入/输出归一化

### 何时需要重新计算？

#### ✅ **需要重新计算的情况**

1. **远程数据集与本地不同**
   ```bash
   # 远程: /home/zhukefei/RoboticsDiffusionTransformer/data/datasets/libero_single_task/
   # 本地: /home/ubuntu/RoboticsDiffusionTransformer/datasets/libero_single_task/
   
   # 如果文件数量、内容、demo数量不同，必须重新计算
   ```

2. **configs/dataset_stat.json 缺少 libero_single_task**
   ```bash
   # 检查是否存在
   grep "libero_single_task" configs/dataset_stat.json
   
   # 如果没有输出，需要重新计算
   ```

3. **统计值异常**
   - 标准差 > 100（通常应该在0.01-10之间）
   - 包含NaN或Inf值
   - 最小值/最大值明显不合理

#### ❌ **不需要重新计算的情况**

1. **数据集完全相同**
   - 文件名、大小、内容都一致
   - 可以用 `md5sum` 验证

2. **统计信息已存在且正确**
   - 已有 `libero_single_task` 条目
   - 数值合理

### 如何验证数据集是否相同？

```bash
# 在本地
cd /home/ubuntu/RoboticsDiffusionTransformer
find datasets/libero_single_task -name "*.hdf5" -exec md5sum {} \; | sort

# 在远程
cd /home/zhukefei/RoboticsDiffusionTransformer
find data/datasets/libero_single_task -name "*.hdf5" -exec md5sum {} \; | sort

# 比较两个输出，如果完全一致，则无需重新计算
```

### 如果需要重新计算

```bash
# 在远程服务器上
cd ~/RoboticsDiffusionTransformer
conda activate rdt

# 运行计算脚本
python compute_single_task_stat.py

# 这会更新 configs/dataset_stat.json
# 添加或更新 libero_single_task 的统计信息
```

---

## 远程训练相对本地的其他关键差异 🔍

### 1. **路径问题** ⭐⭐⭐

#### ✅ 已解决
- [x] 配置文件加载：使用相对路径
- [x] 数据集路径：自动检测
- [x] 训练脚本：路径自动适配

#### ⚠️ 需要注意
```bash
# 检查这些路径是否存在
远程服务器路径检查：
├── ~/RoboticsDiffusionTransformer/data/datasets/libero_single_task/  ✓
├── ~/RoboticsDiffusionTransformer/checkpoints/rdt-1b/model.safetensors  ⚠️
├── ~/RoboticsDiffusionTransformer/google/t5-v1_1-xxl/  ⚠️
└── ~/RoboticsDiffusionTransformer/google/siglip-so400m-patch14-384/  ⚠️
```

**预训练模型和编码器**: 如果不存在，会尝试从HuggingFace下载（很慢！）

---

### 2. **GPU配置** ⭐⭐⭐

#### 本地 vs 远程

| 项目 | 本地 | 远程 |
|------|------|------|
| GPU数量 | 2张 | 8张 |
| GPU型号 | 4090 (48GB) | ? |
| CUDA版本 | ? | ? |
| 脚本配置 | `--num_processes 2` | `--num_processes 8` |

#### 需要检查
```bash
# 在远程服务器
nvidia-smi  # 确认8张GPU都可用
python -c "import torch; print(torch.cuda.device_count())"  # 应输出8
```

#### 脚本已配置
```bash
# train_single_task_2gpu.sh
export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7  # 8张GPU
--num_processes 8
--train_batch_size=4  # 每GPU 4，总batch=32
```

---

### 3. **网络和模型下载** ⭐⭐

#### 可能遇到的问题

1. **HuggingFace访问慢/失败**
   ```bash
   # 解决方案1: 预先下载模型到本地
   # 解决方案2: 使用镜像
   export HF_ENDPOINT=https://hf-mirror.com
   ```

2. **缺少预训练模型**
   ```bash
   # 检查
   ls -lh checkpoints/rdt-1b/model.safetensors
   
   # 如果缺少，需要从本地传输或重新下载
   ```

3. **缺少编码器**
   ```bash
   # 检查
   ls google/t5-v1_1-xxl/
   ls google/siglip-so400m-patch14-384/
   
   # 如果缺少，训练时会自动下载（需要时间和网络）
   ```

#### 建议
```bash
# 在远程服务器上预先检查
cd ~/RoboticsDiffusionTransformer

# 1. 预训练模型
if [ ! -f "checkpoints/rdt-1b/model.safetensors" ]; then
    echo "❌ 缺少预训练模型！"
fi

# 2. 文本编码器
if [ ! -d "google/t5-v1_1-xxl" ]; then
    echo "⚠️  将从HuggingFace下载文本编码器（约10-20分钟）"
fi

# 3. 视觉编码器
if [ ! -d "google/siglip-so400m-patch14-384" ]; then
    echo "⚠️  将从HuggingFace下载视觉编码器（约5分钟）"
fi
```

---

### 4. **环境依赖** ⭐⭐

#### 需要验证的包

| 包名 | 用途 | 检查命令 |
|------|------|----------|
| torch | 深度学习框架 | `python -c "import torch; print(torch.__version__)"` |
| accelerate | 分布式训练 | `python -c "import accelerate; print(accelerate.__version__)"` |
| transformers | 模型库 | `python -c "import transformers; print(transformers.__version__)"` |
| h5py | HDF5读取 | `python -c "import h5py; print(h5py.__version__)"` |
| safetensors | 模型加载 | `python -c "import safetensors; print(safetensors.__version__)"` |

#### 快速检查
```bash
# 在远程服务器
conda activate rdt
python << 'EOF'
import sys
try:
    import torch
    import accelerate
    import transformers
    import h5py
    import safetensors
    print("✓ 所有依赖包已安装")
except ImportError as e:
    print(f"✗ 缺少依赖: {e}")
    sys.exit(1)
EOF
```

---

### 5. **磁盘空间** ⭐⭐

#### 训练需要的空间

| 项目 | 大小估算 |
|------|----------|
| 数据集 | ~100MB-1GB |
| 预训练模型 | ~2-3GB |
| 编码器 (T5 + SigLIP) | ~10-15GB |
| Checkpoint (每个) | ~2-3GB |
| 总checkpoint (20个limit) | ~40-60GB |
| **总计** | **~50-80GB** |

#### 检查命令
```bash
# 在远程服务器
df -h .
# 确保有至少 100GB 可用空间
```

---

### 6. **训练配置差异** ⭐

#### 本地 vs 远程建议配置

| 参数 | 本地(2-GPU) | 远程(8-GPU) | 说明 |
|------|-------------|-------------|------|
| `num_processes` | 2 | 8 | GPU数量 |
| `train_batch_size` | 4 | 4 | 每GPU batch |
| `总batch size` | 8 | 32 | 总batch=num_processes×train_batch_size |
| `gradient_accumulation_steps` | 1 | 1 | 梯度累积 |
| `有效batch size` | 8 | 32 | 总batch×accumulation |
| `learning_rate` | 1e-4 | 1e-4 | 学习率 |
| `max_train_steps` | 30000 | 30000 | 总步数 |
| `预计训练时间` | ~10-12小时 | ~3-4小时 | 估算 |

#### 注意
- 8-GPU训练时，有效batch size是本地的4倍
- 可能需要调整学习率或warmup步数
- 当前配置使用相同的学习率（1e-4），应该没问题

---

### 7. **Weights & Biases (可选)** ⭐

#### 如果使用W&B监控

```bash
# 在远程服务器登录
wandb login

# 或设置离线模式
export WANDB_MODE=offline
```

#### 如果不使用
```bash
# 禁用W&B
export WANDB_DISABLED=true
```

---

### 8. **文件权限** ⭐

#### 需要检查

```bash
# 训练脚本执行权限
chmod +x train_single_task_2gpu.sh

# 输出目录写权限
mkdir -p checkpoints/single_task_scene10_2gpu
ls -ld checkpoints/single_task_scene10_2gpu
```

---

### 9. **进程管理** ⭐⭐

#### 建议使用tmux或screen

```bash
# 安装tmux（如果没有）
sudo apt install tmux  # 或 yum install tmux

# 创建会话
tmux new -s training

# 运行训练
cd ~/RoboticsDiffusionTransformer
conda activate rdt
bash train_single_task_2gpu.sh

# 分离会话: Ctrl+B, 然后按 D
# 重新连接: tmux attach -t training
```

#### 或使用nohup
```bash
nohup bash train_single_task_2gpu.sh > train.log 2>&1 &

# 查看日志
tail -f train.log

# 查看进程
ps aux | grep accelerate
```

---

### 10. **监控和调试** ⭐⭐

#### 训练中应该监控的

1. **GPU利用率**
   ```bash
   watch -n 1 nvidia-smi
   # GPU利用率应该接近100%
   # 显存使用应该稳定
   ```

2. **训练日志**
   ```bash
   tail -f train.log
   # 查看loss变化
   # 检查是否有错误
   ```

3. **Checkpoint保存**
   ```bash
   watch -n 30 "ls -lht checkpoints/single_task_scene10_2gpu/ | head"
   # 每2000步应该保存一个checkpoint
   ```

4. **磁盘空间**
   ```bash
   watch -n 60 "df -h ."
   # 确保不会满盘
   ```

---

## 完整检查清单 📋

### 在开始训练前，在远程服务器上运行：

```bash
# 1. 连接到远程服务器
ssh -J zhukefei@134.175.121.223 zhukefei@172.16.0.27

# 2. 进入项目目录
cd ~/RoboticsDiffusionTransformer

# 3. 激活环境
conda activate rdt

# 4. 运行自动检查脚本
bash remote_training_check.sh
```

### 检查脚本会验证：

- ✓ Python和Conda环境
- ✓ GPU数量和状态
- ✓ 数据集文件存在性
- ✓ **数据集统计信息 (重要!)**
- ✓ 配置文件完整性
- ✓ 预训练模型存在性
- ✓ 输出目录和磁盘空间
- ✓ 训练脚本权限
- ✓ Python依赖包
- ✓ 网络和HuggingFace连接

---

## 统计信息详细说明 📊

### 为什么统计信息这么重要？

```python
# 在训练代码中 (data/hdf5_libero_dataset.py)
# 统计信息用于计算loss权重

# 加载全局统计
with open('configs/dataset_stat.json', 'r') as f:
    global_stats = json.load(f)
    
self.action_std_global = np.array(global_stats[dataset_name]['action_std'])

# 用于计算state_norm (loss权重)
# state_norm = 1.0 / (state_std_global + 1e-5)
# 这确保了不同维度的loss有合适的权重
```

### 统计信息包含什么？

```json
{
  "libero_single_task": {
    "action_mean": [128个值],      // 动作均值
    "action_std": [128个值],        // 动作标准差 ⭐
    "action_min": [128个值],        // 动作最小值
    "action_max": [128个值],        // 动作最大值
    "state_mean": [128个值],        // 状态均值
    "state_std": [128个值],         // 状态标准差 ⭐
    "state_min": [128个值],         // 状态最小值
    "state_max": [128个值]          // 状态最大值
  }
}
```

### 如果统计信息错误会怎样？

❌ **使用错误的统计信息 (如libero_90)**:
- loss权重不正确
- 某些维度被过度惩罚或忽略
- 训练不稳定或收敛到错误的解
- **这就是之前训练效果不好的原因之一！**

✅ **使用正确的统计信息 (libero_single_task)**:
- loss权重合理
- 所有维度平衡学习
- 训练稳定收敛

---

## 快速决策流程 🎯

```
开始
  ↓
远程数据集 == 本地数据集？
  ↓                    ↓
 是                   否
  ↓                    ↓
统计信息已存在？      重新计算统计信息！
  ↓          ↓          ↓
 是         否          ↓
  ↓          ↓          ↓
直接训练   计算统计   使用新统计训练
```

---

## 推荐流程 ⭐

### 第一次在远程服务器训练

```bash
# 1. 连接到服务器
ssh -J zhukefei@134.175.121.223 zhukefei@172.16.0.27

# 2. 进入项目
cd ~/RoboticsDiffusionTransformer
conda activate rdt

# 3. 运行完整检查
bash remote_training_check.sh

# 4. 根据检查结果决定是否重新计算统计
# 如果需要:
python compute_single_task_stat.py

# 5. 开始训练
tmux new -s training
bash train_single_task_2gpu.sh

# 6. 分离会话 (Ctrl+B, D)

# 7. 监控
tmux attach -t training  # 查看训练
nvidia-smi  # 查看GPU
```

---

## 总结 📝

### 最关键的3点

1. **数据集统计信息** - 如果远程数据集不同，必须重新计算
2. **预训练模型路径** - 确保 checkpoints/rdt-1b/model.safetensors 存在
3. **GPU配置** - 脚本已配置为8-GPU，确认服务器有8张可用GPU

### 已经自动处理的

✅ 代码路径问题（相对路径）
✅ 数据集路径自动检测
✅ 训练脚本已配置为8-GPU
✅ 配置文件加载使用相对路径

### 需要手动检查的

⚠️ 数据集是否与本地一致
⚠️ 统计信息是否正确
⚠️ 预训练模型是否存在
⚠️ GPU是否都可用
⚠️ 磁盘空间是否充足

---

**建议**: 先运行 `bash remote_training_check.sh`，根据输出决定下一步行动！

