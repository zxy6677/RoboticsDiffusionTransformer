# RDTå®˜æ–¹ä»£ç çš„æ•°æ®é‡‡æ ·ç­–ç•¥è¯¦è§£

## ğŸ“š åŸºäºä»£ç åˆ†æçš„å®˜æ–¹é‡‡æ ·ç­–ç•¥

### æ ¸å¿ƒå‘ç°ï¼š**å…¨æ­¥éª¤é‡‡æ ·ç­–ç•¥ï¼ˆFull-Step Samplingï¼‰**

RDTå®˜æ–¹ä»£ç **ä¸æ˜¯éšæœºé‡‡æ ·**ï¼Œè€Œæ˜¯é‡‡ç”¨**å…¨æ­¥éª¤æšä¸¾**çš„ç­–ç•¥ï¼

---

## ğŸ” ä»£ç è¯æ®åˆ†æ

### 1. Episode Transformé˜¶æ®µï¼ˆdata/episode_transform.pyï¼‰

```python
# ç¬¬224-299è¡Œï¼šflatten_episodeå‡½æ•°
def flatten_episode(episode: dict) -> tf.data.Dataset:
    """
    Flatten the episode to a list of steps.
    """
    episode_dict = episode['episode_dict']
    dataset_name = episode['dataset_name']
    
    json_content, states, masks = generate_json_state(
        episode_dict, dataset_name
    )

    # ä¸ºæ¯ä¸€æ­¥åˆ›å»ºè®­ç»ƒæ ·æœ¬
    step_data = []
    for i in range(tf.shape(states)[0]):  # éå†æ‰€æœ‰æ­¥éª¤ï¼
        step_data.append({
            'step_id': episode['step_id'][i],
            'json_content': json_content,
            'state_chunk': past_states[i],      # è¿‡å»64æ­¥çš„state
            'action_chunk': future_states[i],    # æœªæ¥64æ­¥çš„action
            # ... å…¶ä»–æ•°æ®
        })
    
    return tf.data.Dataset.from_tensor_slices(step_data)
```

**å…³é”®ç‚¹**ï¼š
- `for i in range(tf.shape(states)[0])` â†’ **éå†episodeçš„æ‰€æœ‰æ­¥éª¤**
- æ¯ä¸ªstepéƒ½åˆ›å»ºä¸€ä¸ªè®­ç»ƒæ ·æœ¬
- æ²¡æœ‰éšæœºé‡‡æ ·ï¼Œè€Œæ˜¯å…¨éƒ¨è½¬æ¢

---

### 2. configs/base.yamlä¸­çš„é…ç½®

```yaml
dataset:
  # è¿‡æ»¤æ‰é•¿åº¦å°äº32çš„episode
  epsd_len_thresh_low: 32
  
  # å¯¹äºè¶…è¿‡2048æ­¥çš„episodeï¼Œéšæœºé‡‡æ ·2048æ­¥
  epsd_len_thresh_high: 2048
  # to better balance the training datasets
```

**é…ç½®è¯´æ˜**ï¼š
- `epsd_len_thresh_low: 32` â†’ è¿‡æ»¤å¤ªçŸ­çš„episode
- `epsd_len_thresh_high: 2048` â†’ é™åˆ¶å¤ªé•¿episodeçš„æ­¥æ•°

**ä½†æ˜¯**ï¼šåœ¨ç°æœ‰ä»£ç ä¸­**æ²¡æœ‰æ‰¾åˆ°å®é™…ä½¿ç”¨è¿™ä¸¤ä¸ªå‚æ•°çš„åœ°æ–¹**ï¼

è¿™è¯´æ˜ï¼š
1. è¿™äº›é…ç½®å¯èƒ½æ˜¯ä¸ºäº†é¢„è®­ç»ƒçš„å¤§è§„æ¨¡æ•°æ®é›†è®¾è®¡çš„
2. åœ¨æˆ‘ä»¬èƒ½çœ‹åˆ°çš„ä»£ç ä¸­ï¼ˆdata/episode_transform.pyï¼‰ï¼Œ**æ‰€æœ‰stepséƒ½è¢«å¤„ç†**

---

### 3. Producer-Consumeræ¨¡å¼ï¼ˆdata/producer.py + train/dataset.pyï¼‰

#### Producerï¼ˆç”Ÿäº§è€…ï¼‰

```python
# data/producer.py ç¬¬185-199è¡Œ
for episode_steps in vla_dataset:
    for step in episode_steps:  # éå†episodeä¸­çš„æ¯ä¸ªstep
        if fill_up and fill_chunk_idx < chunk_end_idx:
            # ä¿å­˜è¿™ä¸ªstepåˆ°buffer
            save_sample(step, chunk_dir, fill_chunk_item_idx)
            # ...
```

**å…³é”®ç‚¹**ï¼š
- Produceréå†episodeçš„æ¯ä¸€ä¸ªstep
- æ¯ä¸ªstepéƒ½ä¼šè¢«ä¿å­˜åˆ°buffer

#### Consumerï¼ˆæ¶ˆè´¹è€…ï¼‰

```python
# train/dataset.py ç¬¬204-246è¡Œ
def _safe_load(self, index):
    read_chunk_idx = index // self.chunk_size
    
    # ä»bufferä¸­è¯»å–ä¸€ä¸ªchunk
    read_chunk_dir = os.path.join(self.buffer_dir, f"chunk_{read_chunk_idx}")
    read_chunk_item_indices = get_clean_item(read_chunk_dir)
    
    # æ ¹æ®indexé€‰æ‹©chunkå†…çš„item
    random_item_index = index % len(read_chunk_item_indices)
    read_chunk_item_index = read_chunk_item_indices[random_item_index]
    
    # åŠ è½½sample
    content, meta = self._load_data_from_chunk(read_chunk_dir, read_chunk_item_index)
    return (content, *meta)
```

**å…³é”®ç‚¹**ï¼š
- Consumeræ ¹æ®DataLoaderçš„indexæ¥è¯»å–steps
- `index % len(read_chunk_item_indices)` â†’ ä¼ªéšæœºè®¿é—®
- ä½†æœ¬è´¨ä¸Šbufferé‡Œå­˜çš„æ˜¯**æ‰€æœ‰episodeçš„æ‰€æœ‰steps**

---

## ğŸ“Š RDTå®˜æ–¹é‡‡æ ·ç­–ç•¥æ€»ç»“

### ç­–ç•¥ç‰¹ç‚¹

| ç‰¹æ€§ | RDTå®˜æ–¹ç­–ç•¥ | æˆ‘ä»¬çš„LIBEROå®ç° |
|------|-----------|----------------|
| **é‡‡æ ·æ–¹å¼** | å…¨æ­¥éª¤æšä¸¾ | éšæœºé‡‡æ ·å•æ­¥ |
| **æ•°æ®è¦†ç›–** | 100%çš„steps | æ¯æ¬¡åªç”¨1æ­¥ |
| **Episodeä½¿ç”¨** | æ¯ä¸ªepisodeçš„æ¯æ­¥éƒ½ç”¨ | æ¯æ¬¡éšæœºé€‰1ä¸ªepisode |
| **è®­ç»ƒæ ·æœ¬æ•°** | episode_len Ã— num_episodes | num_epochs Ã— batch_size |
| **æ•°æ®å¤šæ ·æ€§** | è‡ªç„¶é¡ºåº+DataLoader shuffle | å®Œå…¨éšæœº |

---

### è¯¦ç»†å¯¹æ¯”

#### RDTå®˜æ–¹ç­–ç•¥ï¼ˆå…¨æ­¥éª¤æšä¸¾ï¼‰

```python
# é¢„å¤„ç†é˜¶æ®µï¼ˆç¦»çº¿ï¼‰
for episode in dataset:
    for step_idx in range(len(episode)):
        # æ¯ä¸ªstepéƒ½åˆ›å»ºä¸€ä¸ªè®­ç»ƒæ ·æœ¬
        sample = {
            'state': episode[step_idx],
            'actions': episode[step_idx:step_idx+64],  # æœªæ¥64æ­¥
            'images': episode[step_idx-2:step_idx],    # å†å²2å¸§
            # ...
        }
        save_to_buffer(sample)

# è®­ç»ƒé˜¶æ®µï¼ˆåœ¨çº¿ï¼‰
# DataLoaderä¼šshuffleè¿™äº›samples
# æ¯ä¸ªepochä¼šéå†æ‰€æœ‰samples
```

**ä¼˜ç‚¹**ï¼š
1. âœ… **æ•°æ®åˆ©ç”¨ç‡100%** - æ¯ä¸ªstepéƒ½è¢«ç”¨åˆ°
2. âœ… **è®­ç»ƒç¨³å®š** - å¤§é‡æ ·æœ¬ï¼Œæ›´å¥½çš„æ¢¯åº¦ä¼°è®¡
3. âœ… **è¦†ç›–å®Œæ•´** - å­¦ä¹ åˆ°è½¨è¿¹çš„æ‰€æœ‰é˜¶æ®µ
4. âœ… **é€‚åˆå¤§è§„æ¨¡é¢„è®­ç»ƒ** - 1M+ episodes â†’ æ•°ç™¾ä¸‡steps

**ç¼ºç‚¹**ï¼š
1. âš ï¸ **éœ€è¦å¤§é‡å­˜å‚¨** - bufferå­˜å‚¨æ‰€æœ‰steps
2. âš ï¸ **é¢„å¤„ç†æ—¶é—´é•¿** - éœ€è¦ç¦»çº¿å¤„ç†æ‰€æœ‰æ•°æ®
3. âš ï¸ **å†…å­˜å ç”¨å¤§** - bufferå¯èƒ½éœ€è¦400GB+

---

#### æˆ‘ä»¬çš„LIBEROå®ç°ï¼ˆéšæœºé‡‡æ ·ï¼‰

```python
# data/hdf5_libero_dataset.py
def get_item(self, index: int=None, state_only=False):
    # 1. éšæœºé€‰æ‹©ä¸€ä¸ªepisode
    file_path = np.random.choice(self.file_paths, p=self.episode_sample_weights)
    
    # 2. éšæœºé€‰æ‹©episodeä¸­çš„ä¸€ä¸ªèµ·ç‚¹
    episodes = list(f['data'].keys())
    episode_key = np.random.choice(episodes)
    episode_data = f['data'][episode_key]
    
    # 3. éšæœºé€‰æ‹©èµ·å§‹step
    num_steps = len(actions)
    step_id = np.random.randint(0, num_steps - self.CHUNK_SIZE)
    
    # 4. è¿”å›è¿™ä¸ªstepçš„æ•°æ®
    return sample
```

**ä¼˜ç‚¹**ï¼š
1. âœ… **ä¸éœ€è¦é¢„å¤„ç†** - ç›´æ¥ä»HDF5è¯»å–
2. âœ… **å†…å­˜å ç”¨å°** - åªåŠ è½½éœ€è¦çš„æ•°æ®
3. âœ… **çµæ´»** - å®¹æ˜“ä¿®æ”¹é‡‡æ ·ç­–ç•¥

**ç¼ºç‚¹**ï¼š
1. âŒ **æ•°æ®åˆ©ç”¨ç‡ä½** - æ¯ä¸ªepochåªç”¨å¾ˆå°‘çš„steps
2. âŒ **å¯èƒ½é‡‡æ ·ä¸å‡åŒ€** - æŸäº›stepså¯èƒ½å¾ˆå°‘è¢«é‡‡åˆ°
3. âŒ **å…³é”®æ—¶åˆ»é‡‡æ ·ä¸è¶³** - ä»»åŠ¡è½¬æ¢ç‚¹ï¼ˆå¦‚ç¬¬120æ­¥ï¼‰é‡‡æ ·æ¦‚ç‡ä½

---

## ğŸ¯ ä¸ºä»€ä¹ˆRDTç”¨å…¨æ­¥éª¤æšä¸¾ï¼Ÿ

### ç†ç”±1: å¤§è§„æ¨¡é¢„è®­ç»ƒéœ€è¦

```
RDT-1Bé¢„è®­ç»ƒæ•°æ®ï¼š
- 46ä¸ªæ•°æ®é›†
- 1M+ episodes
- å¹³å‡æ¯ä¸ªepisode: 100-500æ­¥
- æ€»steps: 100M-500M steps

é‡‡æ ·ç­–ç•¥ï¼š
- å…¨æ­¥éª¤æšä¸¾ â†’ 100M-500Mè®­ç»ƒæ ·æœ¬ âœ…
- éšæœºé‡‡æ · â†’ å–å†³äºè®­ç»ƒepochï¼Œå¯èƒ½åªç”¨10M samples âŒ
```

**å¤§è§„æ¨¡é¢„è®­ç»ƒéœ€è¦å……åˆ†åˆ©ç”¨æ‰€æœ‰æ•°æ®ï¼**

### ç†ç”±2: ä¿è¯æ•°æ®è¦†ç›–å‡åŒ€

```python
# å…¨æ­¥éª¤æšä¸¾
for episode in dataset:
    for step in episode:
        # æ¯ä¸ªstepéƒ½ä¼šè¢«ç”¨åˆ°
        # åŒ…æ‹¬ï¼š
        # - ä»»åŠ¡å¼€å§‹ï¼ˆç¬¬0-10æ­¥ï¼‰
        # - ä»»åŠ¡ä¸­æœŸï¼ˆç¬¬50-100æ­¥ï¼‰
        # - ä»»åŠ¡è½¬æ¢ï¼ˆç¬¬100-120æ­¥ï¼‰  â† å…³é”®ï¼
        # - ä»»åŠ¡ç»“æŸï¼ˆç¬¬200-213æ­¥ï¼‰
        save_sample(step)

# éšæœºé‡‡æ ·
for epoch in range(num_epochs):
    step = random_sample()
    # é—®é¢˜ï¼š
    # - ä»»åŠ¡è½¬æ¢ç‚¹ï¼ˆç¬¬100-120æ­¥ï¼‰åªå æ€»æ­¥æ•°çš„10%
    # - è¢«é‡‡æ ·åˆ°çš„æ¦‚ç‡åªæœ‰10%
    # - å¦‚æœè®­ç»ƒä¸å¤Ÿä¹…ï¼Œå¯èƒ½å­¦ä¸åˆ°è½¬æ¢ï¼
```

### ç†ç”±3: è®­ç»ƒç¨³å®šæ€§

```python
# æœ‰æ•ˆbatch size = 256
# å¦‚æœä½¿ç”¨éšæœºé‡‡æ ·ï¼š
#   - æ¯ä¸ªbatchçš„256ä¸ªsampleså¯èƒ½æ¥è‡ªåŒä¸€ä¸ªepisodeçš„ä¸åŒä½ç½®
#   - ç›¸é‚»stepsçš„æ•°æ®åˆ†å¸ƒéå¸¸ç›¸ä¼¼
#   - æ¢¯åº¦ä¼°è®¡ä¸å¤Ÿdiverse

# å¦‚æœä½¿ç”¨å…¨æ­¥éª¤æšä¸¾ + DataLoader shuffleï¼š
#   - æ¯ä¸ªbatchçš„256ä¸ªsamplesæ¥è‡ªä¸åŒepisodes
#   - æ•°æ®åˆ†å¸ƒæ›´diverse
#   - æ¢¯åº¦ä¼°è®¡æ›´å‡†ç¡®
```

---

## ğŸ”¬ configs/base.yamlä¸­çš„epsd_len_threshé…ç½®

### é…ç½®çš„æ„å›¾

```yaml
# è¿‡æ»¤å¤ªçŸ­çš„episodeï¼ˆä¿¡æ¯é‡ä¸è¶³ï¼‰
epsd_len_thresh_low: 32

# å¯¹äºå¤ªé•¿çš„episodeï¼Œéšæœºé‡‡æ ·å›ºå®šæ•°é‡çš„steps
epsd_len_thresh_high: 2048
# to better balance the training datasets
```

### ä¸ºä»€ä¹ˆéœ€è¦thresh_highï¼Ÿ

```python
# é—®é¢˜ï¼šæŸäº›æ•°æ®é›†çš„episodeç‰¹åˆ«é•¿
# ä¾‹å¦‚ï¼š
# - aloha dataset: å¹³å‡200æ­¥/episode
# - bridge dataset: å¹³å‡100æ­¥/episode  
# - robot_play: å¹³å‡5000æ­¥/episode âš ï¸ å¤ªé•¿ï¼

# å¦‚æœä¸é™åˆ¶ï¼š
for episode in robot_play:
    for step in episode:  # 5000 steps
        save(step)
# â†’ robot_playè´¡çŒ®5000ä¸ªsamples
# â†’ alohaåªè´¡çŒ®200ä¸ªsamples
# â†’ è®­ç»ƒä¼šä¸¥é‡åå‘robot_playï¼âŒ

# å¦‚æœé™åˆ¶åˆ°2048ï¼š
for episode in robot_play:
    sampled_steps = random.sample(episode, min(len(episode), 2048))
    for step in sampled_steps:  # æœ€å¤š2048 steps
        save(step)
# â†’ æ¯ä¸ªepisodeæœ€å¤šè´¡çŒ®2048ä¸ªsamples
# â†’ æ•°æ®é›†ä¹‹é—´æ›´å¹³è¡¡ âœ…
```

### ä½†æ˜¯ä»£ç ä¸­æ²¡æœ‰å®ç°ï¼Ÿ

**å¯èƒ½çš„åŸå› **ï¼š
1. **ä»£ç ä¸å®Œæ•´** - è¿™ä¸ªåŠŸèƒ½å¯èƒ½åœ¨å…¶ä»–æ–‡ä»¶ä¸­ï¼ˆæˆ‘ä»¬æ²¡çœ‹åˆ°çš„éƒ¨åˆ†ï¼‰
2. **ä»…ç”¨äºé¢„è®­ç»ƒ** - å¾®è°ƒæ—¶ä¸éœ€è¦è¿™ä¸ªåŠŸèƒ½
3. **é…ç½®é¢„ç•™** - è®¡åˆ’å®ç°ä½†è¿˜æ²¡å®ç°

**åœ¨æˆ‘ä»¬çš„LIBEROå¾®è°ƒä¸­**ï¼š
- Episodeé•¿åº¦éƒ½æ¯”è¾ƒç»Ÿä¸€ï¼ˆ~213æ­¥ï¼‰
- ä¸éœ€è¦thresh_highçš„é™åˆ¶
- ä½¿ç”¨å…¨æ­¥éª¤æšä¸¾å°±å¥½

---

## ğŸ’¡ å¯¹æˆ‘ä»¬LIBEROè®­ç»ƒçš„å¯ç¤º

### å½“å‰é—®é¢˜

```python
# æˆ‘ä»¬çš„å®ç°
def get_item(self):
    # æ¯æ¬¡éšæœºé€‰1ä¸ªepisodeçš„1ä¸ªstep
    return random_sample_one_step()

# è®­ç»ƒé…ç½®
batch_size = 4
gradient_accumulation = 1
GPUs = 8
effective_batch_size = 32

# é—®é¢˜ï¼š
# - 50ä¸ªepisodes Ã— 213æ­¥ = 10,650ä¸ªå¯èƒ½çš„samples
# - æ¯ä¸ªepochåªé‡‡æ ·: 32 Ã— num_batches
# - å¦‚æœnum_batches=100ï¼Œæ¯ä¸ªepochåªç”¨3200ä¸ªsamples
# - æ•°æ®åˆ©ç”¨ç‡ï¼š3200/10650 = 30% âŒ
```

### RDTå®˜æ–¹ç­–ç•¥å¯ç¤º

```python
# å¦‚æœæ”¹ç”¨å…¨æ­¥éª¤æšä¸¾ï¼š
# é¢„å¤„ç†ï¼šæŠŠ50ä¸ªepisodesçš„æ‰€æœ‰10,650æ­¥éƒ½å­˜èµ·æ¥
# è®­ç»ƒï¼šæ¯ä¸ªepochéå†æ‰€æœ‰10,650ä¸ªsamples

# å¥½å¤„ï¼š
# 1. æ•°æ®åˆ©ç”¨ç‡100% âœ…
# 2. æ¯ä¸ªå…³é”®æ—¶åˆ»éƒ½è¢«å­¦åˆ° âœ…
# 3. è®­ç»ƒæ›´ç¨³å®šï¼ˆæ›´å¤šæ ·æœ¬ï¼‰ âœ…

# é…åˆå¤§batch sizeï¼ˆ256ï¼‰ï¼š
# - æ¢¯åº¦ä¼°è®¡æ›´å‡†ç¡®
# - æ”¶æ•›æ›´å¿«æ›´ç¨³å®š
# - æœ€ç»ˆæ€§èƒ½æ›´å¥½
```

---

## ğŸ”§ æ”¹è¿›å»ºè®®

### é€‰é¡¹1: å®ç°å…¨æ­¥éª¤æšä¸¾ï¼ˆæ¨èï¼‰â­â­â­â­â­

```python
# ä¿®æ”¹hdf5_libero_dataset.py
class HDF5LIBERODataset:
    def __init__(self, ...):
        # é¢„å¤„ç†ï¼šå±•å¹³æ‰€æœ‰episodesä¸ºsteps
        self.all_samples = []
        for hdf5_file in self.file_paths:
            with h5py.File(hdf5_file, 'r') as f:
                for episode_key in f['data'].keys():
                    episode = f['data'][episode_key]
                    num_steps = len(episode['actions'])
                    
                    # æ¯ä¸ªstepéƒ½åˆ›å»ºä¸€ä¸ªsample
                    for step_id in range(num_steps - CHUNK_SIZE):
                        sample = {
                            'file_path': hdf5_file,
                            'episode_key': episode_key,
                            'step_id': step_id
                        }
                        self.all_samples.append(sample)
        
        print(f"Total samples: {len(self.all_samples)}")
        # å¯¹äº50 demos: ~10,000 samples
    
    def __len__(self):
        return len(self.all_samples)
    
    def __getitem__(self, index):
        sample_info = self.all_samples[index]
        # æ ¹æ®sample_infoåŠ è½½æ•°æ®
        # ...
```

**ä¼˜ç‚¹**ï¼š
- æ•°æ®åˆ©ç”¨ç‡100%
- ä¸RDTå®˜æ–¹ç­–ç•¥ä¸€è‡´
- è®­ç»ƒæ›´ç¨³å®š

**ç¼ºç‚¹**ï¼š
- éœ€è¦é‡æ„ä»£ç 
- é¢„å¤„ç†æ—¶é—´ç¨é•¿ï¼ˆä½†åªéœ€è¦ä¸€æ¬¡ï¼‰

---

### é€‰é¡¹2: å¢åŠ å…³é”®æ—¶åˆ»é‡‡æ ·æƒé‡ â­â­â­

```python
# ä¿æŒéšæœºé‡‡æ ·ï¼Œä½†å¢åŠ å…³é”®åŒºåŸŸçš„æƒé‡
def sample_step_with_importance(num_steps, chunk_size):
    # å®šä¹‰å…³é”®åŒºåŸŸï¼ˆä»»åŠ¡è½¬æ¢ç‚¹ï¼‰
    transition_start = 100
    transition_end = 140
    
    # åˆ›å»ºé‡‡æ ·æƒé‡
    weights = np.ones(num_steps - chunk_size)
    weights[transition_start:transition_end] *= 3.0  # 3å€æƒé‡
    weights = weights / weights.sum()
    
    # åŠ æƒéšæœºé‡‡æ ·
    step_id = np.random.choice(len(weights), p=weights)
    return step_id
```

**ä¼˜ç‚¹**ï¼š
- ç®€å•ï¼Œä¸éœ€è¦å¤§æ”¹ä»£ç 
- å…³é”®æ—¶åˆ»å­¦ä¹ æ›´å……åˆ†

**ç¼ºç‚¹**ï¼š
- ä»ç„¶ä¸èƒ½ä¿è¯100%è¦†ç›–
- éœ€è¦æ‰‹åŠ¨å®šä¹‰å…³é”®åŒºåŸŸ

---

### é€‰é¡¹3: ä¿æŒå½“å‰+å¢å¤§æœ‰æ•ˆbatch size â­â­â­â­

```python
# ä¸æ”¹é‡‡æ ·ç­–ç•¥ï¼Œä½†å¢å¤§æœ‰æ•ˆbatch size
# train_single_task_improved.sh

--train_batch_size=4 \
--gradient_accumulation_steps=8 \  # ä»1æ”¹ä¸º8
# æœ‰æ•ˆbatch size: 4 Ã— 8 Ã— 8 = 256

# å¥½å¤„ï¼š
# - è™½ç„¶æ¯æ¬¡é‡‡æ ·çš„sampleså°‘
# - ä½†å¤§batch sizeè®©æ¢¯åº¦ä¼°è®¡æ›´å‡†ç¡®
# - è®­ç»ƒæ›´ç¨³å®š
# - å¯èƒ½éƒ¨åˆ†å¼¥è¡¥é‡‡æ ·ä¸è¶³çš„é—®é¢˜
```

**ä¼˜ç‚¹**ï¼š
- æœ€ç®€å•ï¼Œåªæ”¹é…ç½®
- ç«‹å³å¯ç”¨

**ç¼ºç‚¹**ï¼š
- ä¸èƒ½æ ¹æœ¬è§£å†³é‡‡æ ·ä¸è¶³é—®é¢˜
- ä½†èƒ½æ˜¾è‘—æ”¹å–„è®­ç»ƒç¨³å®šæ€§

---

## ğŸ“Š ç­–ç•¥å¯¹æ¯”æ€»ç»“

| ç­–ç•¥ | æ•°æ®åˆ©ç”¨ç‡ | å®ç°å¤æ‚åº¦ | è®­ç»ƒç¨³å®šæ€§ | æ¨èåº¦ |
|------|----------|-----------|-----------|--------|
| **å…¨æ­¥éª¤æšä¸¾** | 100% | é«˜ï¼ˆéœ€é‡æ„ï¼‰ | æœ€é«˜ | â­â­â­â­â­ |
| **å…³é”®æ—¶åˆ»åŠ æƒ** | 30-50% | ä½ | ä¸­ | â­â­â­ |
| **å¤§batch size** | 30% | æœ€ä½ï¼ˆæ”¹é…ç½®ï¼‰ | é«˜ | â­â­â­â­ |
| **å½“å‰éšæœºé‡‡æ ·** | 30% | N/A | ä½ | â­â­ |

---

## ğŸ¯ æœ€ç»ˆå»ºè®®

### çŸ­æœŸæ–¹æ¡ˆï¼ˆç«‹å³å¯è¡Œï¼‰

1. **å¢å¤§batch size** â­â­â­â­â­
   ```bash
   # ä½¿ç”¨train_single_task_improved.sh
   --gradient_accumulation_steps=8
   # æœ‰æ•ˆbatch size: 256
   ```

2. **æµ‹è¯•exec_horizon=16** â­â­â­â­â­
   ```bash
   bash test_dual_camera.sh
   ```

### ä¸­æœŸæ–¹æ¡ˆï¼ˆå¦‚æœéœ€è¦é‡è®­ï¼‰

3. **å®ç°å…¨æ­¥éª¤æšä¸¾** â­â­â­â­â­
   - ä¸RDTå®˜æ–¹ç­–ç•¥ä¸€è‡´
   - æœ€å¤§åŒ–æ•°æ®åˆ©ç”¨
   - é¢„æœŸæ•ˆæœæœ€å¥½

### é•¿æœŸæ–¹æ¡ˆï¼ˆè¿›ä¸€æ­¥ä¼˜åŒ–ï¼‰

4. **æ·»åŠ è¯¾ç¨‹å­¦ä¹ **
   - å…ˆè®­ç»ƒç®€å•é˜¶æ®µï¼ˆå…³æŠ½å±‰ï¼‰
   - å†è®­ç»ƒå¤æ‚é˜¶æ®µï¼ˆæŠ“ç¢—ï¼‰
   - æœ€åè®­ç»ƒå®Œæ•´ä»»åŠ¡

---

## ğŸ“ æ ¸å¿ƒç»“è®º

**RDTå®˜æ–¹é‡‡æ ·ç­–ç•¥ = å…¨æ­¥éª¤æšä¸¾ï¼ˆFull-Step Samplingï¼‰**

- **ä¸æ˜¯**ä»episodeä¸­éšæœºé‡‡æ ·æŸä¸€æ­¥
- **è€Œæ˜¯**å°†æ¯ä¸ªepisodeçš„æ¯ä¸€æ­¥éƒ½ä½œä¸ºè®­ç»ƒæ ·æœ¬
- **ä¼˜åŠ¿**ï¼šæ•°æ®åˆ©ç”¨ç‡100%ï¼Œè®­ç»ƒç¨³å®šï¼Œè¦†ç›–å®Œæ•´
- **é€‚ç”¨**ï¼šå¤§è§„æ¨¡é¢„è®­ç»ƒï¼Œéœ€è¦å……åˆ†åˆ©ç”¨æ‰€æœ‰æ•°æ®

**æˆ‘ä»¬çš„LIBEROå®ç° = éšæœºé‡‡æ ·ï¼ˆRandom Samplingï¼‰**

- **å½“å‰**ï¼šæ¯æ¬¡éšæœºé€‰1ä¸ªepisodeçš„1ä¸ªstep
- **é—®é¢˜**ï¼šæ•°æ®åˆ©ç”¨ç‡ä½ï¼ˆ~30%ï¼‰ï¼Œå…³é”®æ—¶åˆ»é‡‡æ ·ä¸è¶³
- **æ”¹è¿›**ï¼šå®ç°å…¨æ­¥éª¤æšä¸¾ æˆ– å¢å¤§batch size

**å»ºè®®ä¼˜å…ˆçº§**ï¼š
1. ğŸ¥‡ ç«‹å³æµ‹è¯•ï¼šexec_horizon=16
2. ğŸ¥ˆ ç«‹å³æ”¹è¿›ï¼šgradient_accumulation_steps=8
3. ğŸ¥‰ ä¸­æœŸé‡è®­ï¼šå®ç°å…¨æ­¥éª¤æšä¸¾

---

**è¿™å°±æ˜¯ä¸ºä»€ä¹ˆè®ºæ–‡å¼ºè°ƒ"We train on 1M+ episodes"â€”â€”å› ä¸ºä»–ä»¬ç”¨äº†å…¨æ­¥éª¤æšä¸¾ï¼Œå®é™…è®­ç»ƒæ ·æœ¬æ•°æ˜¯å‡ ç™¾ä¸‡åˆ°ä¸Šäº¿ä¸ªstepsï¼** ğŸ¯

