#!/usr/bin/env python3
"""
RDTåœ¨LIBEROä»»åŠ¡ä¸Šçš„è¯„ä¼°è„šæœ¬
"""

import os
import sys
import argparse
import yaml
import torch
import numpy as np
import cv2
from PIL import Image
import json
from datetime import datetime

# æ·»åŠ è·¯å¾„
sys.path.append('/home/zhukefei/LIBERO/libero')
sys.path.append('.')

import libero
from libero import benchmark
from libero.envs import OffScreenRenderEnv

from models.rdt_runner import RDTRunner
from models.multimodal_encoder.t5_encoder import T5Embedder
from models.multimodal_encoder.siglip_encoder import SiglipVisionTower
import importlib.util

# åŠ¨æ€å¯¼å…¥state_vec
spec = importlib.util.spec_from_file_location("state_vec", "configs/state_vec.py")
state_vec_module = importlib.util.module_from_spec(spec)
sys.modules["state_vec"] = state_vec_module
spec.loader.exec_module(state_vec_module)
STATE_VEC_IDX_MAPPING = state_vec_module.STATE_VEC_IDX_MAPPING

class VideoRecorder:
    """è§†é¢‘å½•åˆ¶å™¨"""
    
    def __init__(self, output_path: str, fps: int = 30, width: int = 128, height: int = 128):
        self.output_path = output_path
        self.fps = fps
        self.width = width
        self.height = height
        self.frames = []
        
    def add_frame(self, image: np.ndarray):
        if image is not None:
            if image.shape[:2] != (self.height, self.width):
                image = cv2.resize(image, (self.width, self.height))
            self.frames.append(image)
    
    def save_video(self):
        if not self.frames:
            print("âš ï¸ æ²¡æœ‰å¸§å¯ä»¥ä¿å­˜")
            return
            
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(self.output_path, fourcc, self.fps, (self.width, self.height))
        
        for frame in self.frames:
            if len(frame.shape) == 3 and frame.shape[2] == 3:
                frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
            else:
                frame_bgr = frame
            out.write(frame_bgr)
        
        out.release()
        print(f"ğŸ¥ è§†é¢‘å·²ä¿å­˜åˆ°: {self.output_path}")

class RDTLIBEROModel:
    """RDTæ¨¡å‹åœ¨LIBEROä¸Šçš„æ¨ç†åŒ…è£…å™¨"""
    
    def __init__(self, config_path: str, pretrained_path: str, 
                 text_encoder_path: str, vision_encoder_path: str):
        # åŠ è½½é…ç½®
        with open(config_path, "r") as fp:
            self.config = yaml.safe_load(fp)
        
        # åˆå§‹åŒ–ç¼–ç å™¨
        self._init_encoders(text_encoder_path, vision_encoder_path)
        
        # åˆå§‹åŒ–RDTæ¨¡å‹
        self._init_rdt_model(pretrained_path)
        
    def _init_encoders(self, text_encoder_path: str, vision_encoder_path: str):
        # æ–‡æœ¬ç¼–ç å™¨
        self.text_encoder = T5Embedder(
            device="cuda",
            from_pretrained=text_encoder_path,
            cache_dir=None,
            model_max_length=77,
        )
        
        # è§†è§‰ç¼–ç å™¨
        self.vision_encoder = SiglipVisionTower(
            vision_tower=vision_encoder_path,
            args=self.config,
            delay_load=False,
        )
        
    def _init_rdt_model(self, pretrained_path: str):
        # è®¡ç®—å›¾åƒæ¡ä»¶é•¿åº¦
        patch_size = self.vision_encoder.vision_tower.config.patch_size
        image_size = self.vision_encoder.vision_tower.config.image_size
        num_patches = (image_size // patch_size) ** 2
        
        img_cond_len = (self.config["common"]["img_history_size"] 
                       * self.config["common"]["num_cameras"] 
                       * num_patches)
        
        self.rdt_model = RDTRunner(
            action_dim=self.config["common"]["state_dim"],
            pred_horizon=self.config["common"]["action_chunk_size"],
            config=self.config["model"],
            lang_token_dim=self.config["model"]["lang_token_dim"],
            img_token_dim=self.config["model"]["img_token_dim"],
            state_token_dim=self.config["model"]["state_token_dim"],
            max_lang_cond_len=self.config["dataset"]["tokenizer_max_length"],
            img_cond_len=img_cond_len,
            img_pos_embed_config=[
                ("image", (self.config["common"]["img_history_size"], 
                    self.config["common"]["num_cameras"], 
                    -num_patches)),  
            ],
            lang_pos_embed_config=[
                ("lang", -self.config["dataset"]["tokenizer_max_length"]),
            ],
            dtype=torch.bfloat16,
        )
        
        # åŠ è½½é¢„è®­ç»ƒæƒé‡
        model_file = os.path.join(pretrained_path, "pytorch_model.bin")
        if os.path.exists(model_file):
            checkpoint = torch.load(model_file, map_location="cpu")
            self.rdt_model.load_state_dict(checkpoint, strict=False)
            print(f"âœ… åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {model_file}")
        else:
            print(f"âš ï¸ é¢„è®­ç»ƒæ¨¡å‹ä¸å­˜åœ¨: {model_file}")
            
    def encode_instruction(self, instruction: str) -> torch.Tensor:
        text_embeds, attention_mask = self.text_encoder.get_text_embeddings([instruction])
        return text_embeds
    
    def reset(self):
        device = "cuda"
        weight_dtype = torch.bfloat16
        self.rdt_model.eval()
        self.text_encoder.model.eval()
        self.vision_encoder.vision_tower.eval()
        
        self.rdt_model = self.rdt_model.to(device, dtype=weight_dtype)
        self.text_encoder.model = self.text_encoder.model.to(device, dtype=weight_dtype)
        self.vision_encoder.vision_tower = self.vision_encoder.vision_tower.to(device, dtype=weight_dtype)
        
    def step(self, state: torch.Tensor, images: list, text_embed: torch.Tensor) -> torch.Tensor:
        device = "cuda"
        dtype = torch.bfloat16
        
        # å¤„ç†å›¾åƒ
        background_color = np.array([
            int(x*255) for x in self.vision_encoder.image_processor.image_mean
        ], dtype=np.uint8).reshape(1, 1, 3)
        background_image = np.ones((
            self.vision_encoder.image_processor.size["height"], 
            self.vision_encoder.image_processor.size["width"], 3), dtype=np.uint8
        ) * background_color
        
        image_tensor_list = []
        for image in images:
            if image is None:
                image = Image.fromarray(background_image)
            
            image = self.vision_encoder.image_processor.preprocess(image, return_tensors='pt')['pixel_values'][0]
            image_tensor_list.append(image)

        image_tensor = torch.stack(image_tensor_list, dim=0).to(device, dtype=dtype)
        
        # è·å–å›¾åƒç‰¹å¾
        image_embeds_list = []
        for i in range(image_tensor.shape[0]):
            single_image = image_tensor[i:i+1]
            vision_output = self.vision_encoder.vision_tower(single_image)
            single_embeds = vision_output.last_hidden_state.detach()
            image_embeds_list.append(single_embeds)
        
        image_embeds = torch.cat(image_embeds_list, dim=1)
        
        # é‡å¤å›¾åƒç‰¹å¾ä»¥æ¨¡æ‹Ÿå†å²
        img_history_size = self.config["common"]["img_history_size"]
        if img_history_size > 1:
            image_embeds = image_embeds.repeat(1, img_history_size, 1)
        
        image_embeds = image_embeds.reshape(1, -1, self.vision_encoder.vision_tower.config.hidden_size)
        
        # å¤„ç†çŠ¶æ€
        joints = state.to(device).unsqueeze(0)
        states = joints.to(device, dtype=dtype).unsqueeze(1)
        
        # åˆ›å»ºçŠ¶æ€æ©ç 
        state_elem_mask = torch.ones(
            (1, 1, self.config["model"]["state_token_dim"]),
            device=device, dtype=dtype
        )
        
        ctrl_freqs = torch.tensor([20]).to(device)
        text_embeds = text_embed.to(device, dtype=dtype)
        
        with torch.no_grad():
            trajectory = self.rdt_model.predict_action(
                lang_tokens=text_embeds,
                lang_attn_mask=torch.ones(
                    text_embeds.shape[:2], dtype=torch.bool,
                    device=text_embeds.device),
                img_tokens=image_embeds,
                state_tokens=states,
                action_mask=state_elem_mask,  
                ctrl_freqs=ctrl_freqs
            )
        
        return trajectory.to(torch.float32)

def convert_libero_state_to_rdt(obs: dict, state_dim: int = 128) -> torch.Tensor:
    """å°†LIBEROè§‚å¯Ÿè½¬æ¢ä¸ºRDTçŠ¶æ€æ ¼å¼"""
    # æå–LIBEROçŠ¶æ€
    joint_pos = obs["robot0_joint_pos"]
    gripper_pos = obs["robot0_gripper_qpos"]
    eef_pos = obs["robot0_eef_pos"]
    eef_quat = obs["robot0_eef_quat"]
    
    # è®¡ç®—gripperçŠ¶æ€
    gripper_state = np.mean(gripper_pos)
    
    # å°†å››å…ƒæ•°è½¬æ¢ä¸º6Dæ—‹è½¬è¡¨ç¤º
    def quat_to_6d_rotation(quat):
        from scipy.spatial.transform import Rotation
        r = Rotation.from_quat(quat)
        rot_matrix = r.as_matrix()
        return rot_matrix[:, :2].flatten()
    
    eef_ori_6d = quat_to_6d_rotation(eef_quat)
    
    # æ„å»º17ç»´LIBEROçŠ¶æ€å‘é‡
    libero_state = np.concatenate([
        joint_pos,
        [gripper_state],
        eef_pos,
        eef_ori_6d
    ])
    
    # æ˜ å°„åˆ°RDTçš„128ç»´çŠ¶æ€ç©ºé—´
    rdt_state = np.zeros(state_dim)
    
    # ä½¿ç”¨å³è‡‚çš„ç´¢å¼•æ˜ å°„
    right_arm_indices = [
        STATE_VEC_IDX_MAPPING[f"right_arm_joint_{i}_pos"] for i in range(7)
    ] + [
        STATE_VEC_IDX_MAPPING["right_gripper_open"]
    ] + [
        STATE_VEC_IDX_MAPPING["right_eef_pos_x"],
        STATE_VEC_IDX_MAPPING["right_eef_pos_y"], 
        STATE_VEC_IDX_MAPPING["right_eef_pos_z"]
    ] + [
        STATE_VEC_IDX_MAPPING["right_eef_angle_0"],
        STATE_VEC_IDX_MAPPING["right_eef_angle_1"],
        STATE_VEC_IDX_MAPPING["right_eef_angle_2"],
        STATE_VEC_IDX_MAPPING["right_eef_angle_3"],
        STATE_VEC_IDX_MAPPING["right_eef_angle_4"],
        STATE_VEC_IDX_MAPPING["right_eef_angle_5"]
    ]
    
    min_len = min(len(libero_state), len(right_arm_indices))
    rdt_state[right_arm_indices[:min_len]] = libero_state[:min_len]
    
    # åŠ è½½æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯è¿›è¡Œå½’ä¸€åŒ–
    with open('configs/dataset_stat.json', 'r') as f:
        stats = json.load(f)
    libero_stats = stats['libero_90']
    
    state_mean = np.array(libero_stats["state_mean"])
    state_std = np.array(libero_stats["state_std"])
    state_std = np.where(state_std == 0, 1.0, state_std)
    rdt_state = (rdt_state - state_mean) / state_std
    
    return torch.from_numpy(rdt_state).float()

def convert_rdt_action_to_libero(rdt_action: torch.Tensor) -> np.ndarray:
    """å°†RDTåŠ¨ä½œè½¬æ¢ä¸ºLIBEROåŠ¨ä½œæ ¼å¼"""
    action_128d = rdt_action[0, 0, :].cpu().numpy()
    
    # åŠ è½½æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯è¿›è¡Œåå½’ä¸€åŒ–
    with open('configs/dataset_stat.json', 'r') as f:
        stats = json.load(f)
    libero_stats = stats['libero_90']
    
    # æå–ä½ç½®
    pos_x_idx = STATE_VEC_IDX_MAPPING["right_eef_pos_x"]
    pos_y_idx = STATE_VEC_IDX_MAPPING["right_eef_pos_y"]
    pos_z_idx = STATE_VEC_IDX_MAPPING["right_eef_pos_z"]
    
    pos_x = action_128d[pos_x_idx] * libero_stats["state_std"][pos_x_idx] + libero_stats["state_mean"][pos_x_idx]
    pos_y = action_128d[pos_y_idx] * libero_stats["state_std"][pos_y_idx] + libero_stats["state_mean"][pos_y_idx]
    pos_z = action_128d[pos_z_idx] * libero_stats["state_std"][pos_z_idx] + libero_stats["state_mean"][pos_z_idx]
    
    # æå–6Dæ—‹è½¬
    ori_indices = [
        STATE_VEC_IDX_MAPPING["right_eef_angle_0"],
        STATE_VEC_IDX_MAPPING["right_eef_angle_1"],
        STATE_VEC_IDX_MAPPING["right_eef_angle_2"],
        STATE_VEC_IDX_MAPPING["right_eef_angle_3"],
        STATE_VEC_IDX_MAPPING["right_eef_angle_4"],
        STATE_VEC_IDX_MAPPING["right_eef_angle_5"]
    ]
    
    ori_6d = np.array([
        action_128d[idx] * libero_stats["state_std"][idx] + libero_stats["state_mean"][idx]
        for idx in ori_indices
    ])
    
    # å°†6Dæ—‹è½¬è½¬æ¢ä¸º3Dæ¬§æ‹‰è§’
    def rotation_6d_to_euler(rot_6d):
        r1 = rot_6d[:3]
        r2 = rot_6d[3:]
        
        r1 = r1 / (np.linalg.norm(r1) + 1e-8)
        r2 = r2 / (np.linalg.norm(r2) + 1e-8)
        
        r3 = np.cross(r1, r2)
        r3 = r3 / (np.linalg.norm(r3) + 1e-8)
        
        rot_matrix = np.column_stack([r1, r2, r3])
        
        from scipy.spatial.transform import Rotation
        r = Rotation.from_matrix(rot_matrix)
        euler = r.as_euler('xyz', degrees=False)
        return euler
    
    ori_3d = rotation_6d_to_euler(ori_6d)
    
    # æå–gripperçŠ¶æ€
    gripper_idx = STATE_VEC_IDX_MAPPING["right_gripper_open"]
    gripper_normalized = action_128d[gripper_idx] * libero_stats["state_std"][gripper_idx] + libero_stats["state_mean"][gripper_idx]
    gripper = gripper_normalized * 2.0 - 1.0
    
    # æ„å»ºLIBEROåŠ¨ä½œå‘é‡
    libero_action = np.array([pos_x, pos_y, pos_z, ori_3d[0], ori_3d[1], ori_3d[2], gripper])
    
    # å°†åŠ¨ä½œç¼©æ”¾åˆ°LIBEROæœŸæœ›çš„[-1, 1]èŒƒå›´
    libero_action[0] = np.clip(-libero_action[0] / 0.05, -1.0, 1.0)
    libero_action[1] = np.clip(libero_action[1] / 0.05, -1.0, 1.0)
    libero_action[2] = np.clip(-libero_action[2] / 0.05, -1.0, 1.0)
    libero_action[3:6] = np.clip(libero_action[3:6] / 0.5, -1.0, 1.0)
    
    return libero_action

def evaluate_rdt_on_libero(model: RDTLIBEROModel, 
                          benchmark_name: str = "libero_90",
                          num_tasks: int = 1,
                          max_steps: int = 100,
                          record_video: bool = False,
                          video_output_dir: str = "videos") -> dict:
    """åœ¨LIBEROåŸºå‡†ä¸Šè¯„ä¼°RDTæ¨¡å‹"""
    
    print(f"ğŸš€ å¼€å§‹RDTåœ¨{benchmark_name}ä¸Šçš„è¯„ä¼°")
    
    # åˆ›å»ºè§†é¢‘è¾“å‡ºç›®å½•
    if record_video:
        os.makedirs(video_output_dir, exist_ok=True)
        print(f"ğŸ¥ è§†é¢‘å½•åˆ¶å·²å¯ç”¨ï¼Œè¾“å‡ºç›®å½•: {video_output_dir}")
    
    # è®¾ç½®LIBEROç¯å¢ƒ
    libero.set_libero_default_path("/home/ubuntu/LIBERO/libero/libero")
    
    # è·å–åŸºå‡†
    benchmark_dict = benchmark.get_benchmark_dict()
    libero_benchmark = benchmark_dict[benchmark_name]()
    
    results = {
        "total_tasks": min(num_tasks, len(libero_benchmark.get_task_names())),
        "successful_tasks": 0,
        "total_steps": 0,
        "task_results": []
    }
    
    # è¯„ä¼°æŒ‡å®šæ•°é‡çš„ä»»åŠ¡
    for task_idx in range(results["total_tasks"]):
        task_name = libero_benchmark.get_task_names()[task_idx]
        print(f"\nğŸ“‹ è¯„ä¼°ä»»åŠ¡ {task_idx+1}/{results['total_tasks']}: {task_name}")
        
        # åˆå§‹åŒ–è§†é¢‘å½•åˆ¶å™¨
        video_recorder = None
        if record_video:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            video_filename = f"task_{task_idx+1:02d}_{task_name[:30]}_{timestamp}.mp4"
            video_path = os.path.join(video_output_dir, video_filename)
            video_recorder = VideoRecorder(video_path, fps=10, width=128, height=128)
            print(f"  ğŸ¥ å¼€å§‹å½•åˆ¶è§†é¢‘: {video_filename}")
        
        try:
            # è·å–ä»»åŠ¡
            task = libero_benchmark.get_task(task_idx)
            
            # åˆ›å»ºç¯å¢ƒ
            bddl_files_path = libero.get_libero_path("bddl_files")
            bddl_file_path = os.path.join(bddl_files_path, task.problem_folder, task.bddl_file)
            
            env_args = {
                "bddl_file_name": bddl_file_path,
                "camera_heights": 128,
                "camera_widths": 128
            }
            
            env = OffScreenRenderEnv(**env_args)
            
            # é‡ç½®ç¯å¢ƒ
            obs = env.reset()
            
            # è®¾ç½®åˆå§‹çŠ¶æ€
            init_states = libero_benchmark.get_task_init_states(task_idx)
            if len(init_states) > 0:
                env.set_init_state(init_states[0])
            
            # ç¼–ç ä»»åŠ¡æè¿°
            text_embed = model.encode_instruction(task_name)
            print(f"  ğŸ“ ä»»åŠ¡æè¿°: {task_name}")
            
            # é‡ç½®æ¨¡å‹
            model.reset()
            
            # è¿è¡Œæ¨ç†
            task_success = False
            task_steps = 0
            
            print(f"  ğŸ¯ å¼€å§‹æ¨ç†å¾ªç¯ï¼Œæœ€å¤§æ­¥æ•°: {max_steps}")
            
            for step in range(max_steps):
                print(f"    ğŸ“ æ­¥éª¤ {step+1}:")
                
                # è®¾ç½®éšæœºç§å­
                torch.manual_seed(task_idx * 1000 + step)
                np.random.seed(task_idx * 1000 + step)
                
                # å‡†å¤‡å›¾åƒè¾“å…¥
                img = obs["agentview_image"]
                images = [Image.fromarray(img), None, None]
                
                # å½•åˆ¶è§†é¢‘å¸§
                if video_recorder is not None:
                    video_recorder.add_frame(img)
                
                # è½¬æ¢çŠ¶æ€
                rdt_state = convert_libero_state_to_rdt(obs)
                
                # æ¨¡å‹æ¨ç†
                rdt_actions = model.step(rdt_state, images, text_embed)
                
                # è½¬æ¢åŠ¨ä½œ
                libero_action = convert_rdt_action_to_libero(rdt_actions)
                
                # æ‰§è¡ŒåŠ¨ä½œ
                obs, reward, done, info = env.step(libero_action)
                task_steps += 1
                
                print(f"      ğŸ“Š å¥–åŠ±: {reward:.3f}, å®Œæˆ: {done}")
                if 'success' in info:
                    print(f"      ğŸ¯ æˆåŠŸçŠ¶æ€: {info['success']}")
                
                if done:
                    task_success = info.get("success", False)
                    print(f"      ğŸ Episodeç»“æŸ: æˆåŠŸ={task_success}")
                    break
            
            # è®°å½•ç»“æœ
            task_result = {
                "task_name": task_name,
                "success": task_success,
                "steps": task_steps,
                "reward": reward
            }
            results["task_results"].append(task_result)
            
            if task_success:
                results["successful_tasks"] += 1
                print(f"  âœ… ä»»åŠ¡æˆåŠŸå®Œæˆï¼Œæ­¥æ•°: {task_steps}")
            else:
                print(f"  âŒ ä»»åŠ¡å¤±è´¥ï¼Œæ­¥æ•°: {task_steps}")
            
            # ä¿å­˜è§†é¢‘
            if video_recorder is not None:
                video_recorder.save_video()
            
            results["total_steps"] += task_steps
            
            env.close()
            
        except Exception as e:
            print(f"  âŒ ä»»åŠ¡æ‰§è¡Œå‡ºé”™: {e}")
            results["task_results"].append({
                "task_name": task_name,
                "success": False,
                "steps": 0,
                "reward": 0,
                "error": str(e)
            })
    
    # è®¡ç®—æˆåŠŸç‡
    results["success_rate"] = results["successful_tasks"] / results["total_tasks"]
    results["avg_steps"] = results["total_steps"] / results["total_tasks"]
    
    print(f"\nğŸ“Š è¯„ä¼°ç»“æœ:")
    print(f"  æ€»ä»»åŠ¡æ•°: {results['total_tasks']}")
    print(f"  æˆåŠŸä»»åŠ¡æ•°: {results['successful_tasks']}")
    print(f"  æˆåŠŸç‡: {results['success_rate']:.2%}")
    print(f"  å¹³å‡æ­¥æ•°: {results['avg_steps']:.1f}")
    
    return results

def main():
    parser = argparse.ArgumentParser(description="RDTåœ¨LIBEROä¸Šçš„è¯„ä¼°")
    parser.add_argument("--config", type=str, default="configs/base.yaml", help="RDTé…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--pretrained", type=str, default="checkpoints/rdt-1b", help="é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„")
    parser.add_argument("--text_encoder", type=str, default="google/t5-v1_1-xxl", help="æ–‡æœ¬ç¼–ç å™¨è·¯å¾„")
    parser.add_argument("--vision_encoder", type=str, default="google/siglip-so400m-patch14-384", help="è§†è§‰ç¼–ç å™¨è·¯å¾„")
    parser.add_argument("--benchmark", type=str, default="libero_90", help="LIBEROåŸºå‡†åç§°")
    parser.add_argument("--num_tasks", type=int, default=1, help="è¯„ä¼°ä»»åŠ¡æ•°é‡")
    parser.add_argument("--max_steps", type=int, default=100, help="æ¯ä¸ªä»»åŠ¡æœ€å¤§æ­¥æ•°")
    parser.add_argument("--record_video", action="store_true", help="æ˜¯å¦å½•åˆ¶è§†é¢‘")
    parser.add_argument("--video_output_dir", type=str, default="videos", help="è§†é¢‘è¾“å‡ºç›®å½•")
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–æ¨¡å‹
    print("ğŸ¤– åˆå§‹åŒ–RDTæ¨¡å‹...")
    model = RDTLIBEROModel(
        config_path=args.config,
        pretrained_path=args.pretrained,
        text_encoder_path=args.text_encoder,
        vision_encoder_path=args.vision_encoder
    )
    
    # è¿è¡Œè¯„ä¼°
    results = evaluate_rdt_on_libero(
        model=model,
        benchmark_name=args.benchmark,
        num_tasks=args.num_tasks,
        max_steps=args.max_steps,
        record_video=args.record_video,
        video_output_dir=args.video_output_dir
    )
    
    # ä¿å­˜ç»“æœ
    with open("libero_evaluation_results.json", "w") as f:
        json.dump(results, f, indent=2)
    
    print(f"\nğŸ’¾ è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: libero_evaluation_results.json")

if __name__ == "__main__":
    main()


